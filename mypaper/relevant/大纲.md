

---

# 第二章 相关理论与技术基础

## 2.1 软件工程造价与敏捷开发度量
*(本节保持不变，确立业务规则)*
### 2.1.1 软件开发成本度量规范 (GB/T 36964)
*   简述标准框架与“先规模后工作量”原则。
### 2.1.2 IFPUG 功能点分析方法
*   简述五类功能组件定义。
*   指出传统人工计数在敏捷环境下的痛点（主观、低效）。
### 2.1.3 敏捷开发环境下的估算特性
*   敏捷迭代对估算速度与灵活性的要求。

## 2.2 大语言模型与推理范式演进
*(本节重点：只讲“通用原理”，不讲“您的具体实现”。即介绍 GoT 是谁提出的，基本思想是什么，而不是讲您怎么设计 Prompt)*

### 2.2.1 大语言模型 (LLMs) 基础
*   简述 Transformer 架构与注意力机制（解释模型具备语义理解能力的来源）。
*   LLM 在软件工程任务（SE）中的应用现状简述。

### 2.2.2 从思维链到思维图谱
*   **思维链 (CoT)：** 介绍 Wei 等人提出的线性推理概念及其局限（难以处理复杂分支）。
*   **思维图谱 (GoT) 的概念模型：**
    *   引用 Besta 等人的原始定义：将推理过程建模为图（Graph），节点为思维状态，边为依赖。
    *   **核心优势：** 仅从理论层面说明图结构相比链式结构更适合处理非线性、多约束问题。
    *   *(注意：这里**不要**写具体的“发散-验证-聚合”节点设计，那是第三章的内容)*。

## 2.3 特征选择与强化学习技术
*(本节旨在构建“问题-工具-解决方案”的理论闭环：先讲特征选择是干什么的，再讲RL的基本原理，最后讲RL如何从理论上解决特征选择的难题)*

### 2.3.1 特征选择方法概述 (Feature Selection Methods)
*   **基本概念：** 定义特征选择（Feature Selection），即从原始特征中选择最优子集的过程，目的是降维、去噪、提升模型泛化能力。
*   **主流方法分类与局限：**
    *   **过滤法 (Filter)：** 基于统计指标（如方差、相关系数）。*评价：速度快，但忽略了特征间的交互和后续模型的反馈。*
    *   **包装法 (Wrapper)：** 基于特定模型的预测性能（如递归特征消除 RFE）。*评价：精度高，但计算开销大，且贪心搜索策略容易陷入局部最优（Local Optima）。*
    *   **嵌入法 (Embedded)：** 集成在算法内部（如 Lasso 回归）。
*   **敏捷环境下的挑战：** 软件工程数据通常高维、稀疏且存在多重共线性，传统方法难以同时兼顾效率与全局最优。

### 2.3.2 强化学习基本原理 (Reinforcement Learning Basics)
*   **核心定义：** 介绍智能体（Agent）、环境（Environment）、奖励（Reward）的交互机制。
*   **马尔科夫决策过程 (MDP)：** 简述五元组 $(S, A, P, R, \gamma)$ 的数学定义。
*   **Q-Learning 与 DQN：**
    *   Q值（动作价值函数）的概念。
    *   深度Q网络（DQN）如何利用神经网络处理高维状态空间（这是后续使用 MARLFS 的基础）。

### 2.3.3 强化学习在特征选择中的应用机制 (RL for Feature Selection)
*   **问题映射 (Mapping)：** 从理论层面解释如何将“特征选择问题”转化为“序列决策问题”。
    *   **状态 (State)：** 当前已选的特征子集。
    *   **动作 (Action)：** 选择或剔除某个特征。
    *   **奖励 (Reward)：** 预测模型的评估指标（如精度提升）。
*   **多智能体强化学习 (MARL) 的优势：**
    *   相比单智能体，多智能体（每个特征作为一个 Agent）在处理高维特征空间时，能通过协作与博弈更好地跳出局部最优解。
    *   *(注：此处仅通过理论对比 highlighting MARL 比 Wrapper 方法更适合复杂空间，为第四章的算法改进做铺垫)*。
## 2.4 本章小结
*   总结：IFPUG 提供了度量标准，GoT 提供了处理复杂规则的认知框架，RL 提供了动态寻优的算法基础。三者结合为本文方法提供了坚实的理论支撑。
