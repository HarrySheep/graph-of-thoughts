import os
import sys
import json
import re
from typing import List, Dict, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ graph_of_thoughts
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))

# LLMç›¸å…³é…ç½®
_LLM_INSTANCE = None
_USE_LLM_SEMANTIC = True

def init_llm(model_name: str = "deepseek", use_semantic: bool = True):
    """
    åˆå§‹åŒ–ç”¨äºè¯­ä¹‰æ¯”è¾ƒçš„LLMå®ä¾‹ã€‚
    
    :param model_name: æ¨¡å‹åç§°
    :param use_semantic: æ˜¯å¦å¯ç”¨LLMè¯­ä¹‰æ¯”è¾ƒ
    """
    global _LLM_INSTANCE, _USE_LLM_SEMANTIC
    
    if use_semantic:
        try:
            from graph_of_thoughts import language_models
            config_path = os.path.join(
                os.path.dirname(__file__),
                "../../graph_of_thoughts/language_models/config.json"
            )
            _LLM_INSTANCE = language_models.ChatGPT(config_path, model_name=model_name, cache=True)
            _USE_LLM_SEMANTIC = True
            print(f"âœ… LLMè¯­ä¹‰æ¯”è¾ƒå·²å¯ç”¨ (æ¨¡å‹: {model_name})")
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åˆå§‹åŒ–LLMï¼Œå›é€€åˆ°å­—ç¬¦ä¸²ç›¸ä¼¼åº¦: {e}")
            _USE_LLM_SEMANTIC = False
    else:
        _USE_LLM_SEMANTIC = False
        print("â„¹ï¸ ä½¿ç”¨å­—ç¬¦ä¸²ç›¸ä¼¼åº¦æ¯”è¾ƒï¼ˆæœªå¯ç”¨LLMï¼‰")

def normalize_name(name: str) -> str:
    """
    æ ‡å‡†åŒ–åŠŸèƒ½ç‚¹åç§°ï¼Œç”¨äºæ¯”è¾ƒã€‚
    """
    name = name.lower().strip()
    name = ' '.join(name.split())
    name = re.sub(r'\([^)]*\)', '', name)
    name = re.sub(r'ï¼ˆ[^ï¼‰]*ï¼‰', '', name)
    return name.strip()

def string_similarity(s1: str, s2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ï¼ˆåŸºäºJaccardç›¸ä¼¼åº¦ï¼‰ã€‚
    """
    if not s1 or not s2:
        return 0.0
    
    set1 = set(s1)
    set2 = set(s2)
    
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    
    return intersection / union if union > 0 else 0.0

def llm_semantic_similarity(name1: str, name2: str) -> float:
    """
    ä½¿ç”¨LLMåˆ¤æ–­ä¸¤ä¸ªåŠŸèƒ½ç‚¹åç§°çš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚
    
    :param name1: ç¬¬ä¸€ä¸ªåŠŸèƒ½ç‚¹åç§°
    :param name2: ç¬¬äºŒä¸ªåŠŸèƒ½ç‚¹åç§°
    :return: ç›¸ä¼¼åº¦åˆ†æ•° (0.0 - 1.0)
    """
    global _LLM_INSTANCE
    
    if _LLM_INSTANCE is None:
        return string_similarity(normalize_name(name1), normalize_name(name2))
    
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªIFPUGåŠŸèƒ½ç‚¹åˆ†æä¸“å®¶ã€‚è¯·åˆ¤æ–­ä»¥ä¸‹ä¸¤ä¸ªåŠŸèƒ½ç‚¹åç§°æ˜¯å¦æŒ‡ä»£åŒä¸€ä¸ªæˆ–éå¸¸ç›¸ä¼¼çš„åŠŸèƒ½ç‚¹ã€‚

åŠŸèƒ½ç‚¹1: {name1}
åŠŸèƒ½ç‚¹2: {name2}

è¯·åˆ†æï¼š
1. å®ƒä»¬æ˜¯å¦æŒ‡ä»£ç›¸åŒæˆ–ç›¸ä¼¼çš„æ•°æ®/åŠŸèƒ½ï¼Ÿ
2. è€ƒè™‘åŒä¹‰è¯ã€ç¼©å†™ã€ä¸­è‹±æ–‡ç¿»è¯‘ç­‰å› ç´ 
3. åªè¦æ ¸å¿ƒè¯­ä¹‰ç›¸åŒå³å¯ï¼Œä¸éœ€è¦å®Œå…¨å­—é¢åŒ¹é…

è¯·ç›´æ¥å›ç­”ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0.0åˆ°1.0ä¹‹é—´çš„å°æ•°ï¼‰ï¼š
- 1.0: å®Œå…¨ç›¸åŒçš„åŠŸèƒ½ç‚¹
- 0.8-0.9: é«˜åº¦ç›¸ä¼¼ï¼Œå¾ˆå¯èƒ½æ˜¯åŒä¸€ä¸ªåŠŸèƒ½ç‚¹
- 0.5-0.7: ä¸­ç­‰ç›¸ä¼¼ï¼Œæœ‰ä¸€å®šå…³è”
- 0.0-0.4: ä¸ç›¸ä¼¼æˆ–ä¸ç›¸å…³

åªéœ€è¦å›ç­”ä¸€ä¸ªæ•°å­—ï¼Œæ ¼å¼å¦‚ï¼š0.85"""

    try:
        response = _LLM_INSTANCE.query(prompt, num_responses=1)
        texts = _LLM_INSTANCE.get_response_texts(response)
        
        if texts and len(texts) > 0:
            text = texts[0].strip()
            match = re.search(r'(\d+\.?\d*)', text)
            if match:
                score = float(match.group(1))
                return max(0.0, min(1.0, score))
    except Exception as e:
        print(f"    âš ï¸ LLMè°ƒç”¨å¤±è´¥: {e}")
    
    # å›é€€åˆ°å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
    return string_similarity(normalize_name(name1), normalize_name(name2))

def get_similarity(name1: str, name2: str) -> float:
    """
    è·å–ä¸¤ä¸ªåç§°çš„ç›¸ä¼¼åº¦ï¼Œæ ¹æ®é…ç½®é€‰æ‹©LLMæˆ–å­—ç¬¦ä¸²æ–¹æ³•ã€‚
    """
    if _USE_LLM_SEMANTIC and _LLM_INSTANCE:
        return llm_semantic_similarity(name1, name2)
    else:
        return string_similarity(normalize_name(name1), normalize_name(name2))

def calculate_semantic_similarity(predicted: List[str], ground_truth: List[str], 
                                   similarity_threshold: float = 0.5,
                                   verbose: bool = False) -> Dict:
    """
    è®¡ç®—é¢„æµ‹çš„åŠŸèƒ½ç‚¹åˆ—è¡¨å’ŒçœŸå®ç­”æ¡ˆçš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚
    ä½¿ç”¨ç²¾ç¡®åŒ¹é… + æ¨¡ç³Š/è¯­ä¹‰åŒ¹é…ç›¸ç»“åˆçš„æ–¹æ³•ã€‚
    """
    if not ground_truth and not predicted:
        return {
            "f1_score": 1.0,
            "precision": 1.0,
            "recall": 1.0,
            "exact_matches": [],
            "fuzzy_matches": [],
            "unmatched_predicted": [],
            "unmatched_ground_truth": []
        }
    if not ground_truth or not predicted:
        return {
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0,
            "exact_matches": [],
            "fuzzy_matches": [],
            "unmatched_predicted": predicted if predicted else [],
            "unmatched_ground_truth": ground_truth if ground_truth else []
        }
    
    # æ ‡å‡†åŒ–æ‰€æœ‰åç§°
    pred_normalized = [(normalize_name(p), p) for p in predicted]
    truth_normalized = [(normalize_name(t), t) for t in ground_truth]
    
    # ç²¾ç¡®åŒ¹é…
    pred_norm_set = set(p[0] for p in pred_normalized)
    truth_norm_set = set(t[0] for t in truth_normalized)
    
    exact_match_norms = pred_norm_set & truth_norm_set
    exact_matches = []
    
    for norm in exact_match_norms:
        pred_orig = next((p[1] for p in pred_normalized if p[0] == norm), None)
        truth_orig = next((t[1] for t in truth_normalized if t[0] == norm), None)
        if pred_orig and truth_orig:
            exact_matches.append({
                "predicted": pred_orig,
                "ground_truth": truth_orig,
                "score": 1.0
            })
    
    # è¯­ä¹‰/æ¨¡ç³ŠåŒ¹é…
    unmatched_pred = [(norm, orig) for norm, orig in pred_normalized if norm not in exact_match_norms]
    unmatched_truth = [(norm, orig) for norm, orig in truth_normalized if norm not in exact_match_norms]
    
    fuzzy_score = 0.0
    matched_truth_origs = set()
    fuzzy_matches = []
    
    for pred_norm, pred_orig in unmatched_pred:
        max_similarity = 0.0
        best_match_orig = None
        
        if verbose:
            print(f"    ğŸ” åŒ¹é… '{pred_orig}'...")
        
        for truth_norm, truth_orig in unmatched_truth:
            if truth_orig in matched_truth_origs:
                continue
            
            # ä½¿ç”¨LLMæˆ–å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
            similarity = get_similarity(pred_orig, truth_orig)
            
            if verbose:
                print(f"       vs '{truth_orig}': {similarity:.2f}")
            
            if similarity > max_similarity:
                max_similarity = similarity
                best_match_orig = truth_orig
        
        if max_similarity >= similarity_threshold and best_match_orig:
            fuzzy_score += max_similarity
            matched_truth_origs.add(best_match_orig)
            fuzzy_matches.append({
                "predicted": pred_orig,
                "ground_truth": best_match_orig,
                "score": round(max_similarity, 2)
            })
            if verbose:
                print(f"       âœ“ åŒ¹é…: {pred_orig} â†” {best_match_orig} ({max_similarity:.2f})")
    
    # æœªåŒ¹é…çš„é¡¹
    final_unmatched_pred = [orig for norm, orig in unmatched_pred 
                           if orig not in [m["predicted"] for m in fuzzy_matches]]
    final_unmatched_truth = [orig for norm, orig in unmatched_truth 
                            if orig not in matched_truth_origs]
    
    # è®¡ç®—F1åˆ†æ•°
    total_matches = len(exact_matches) + fuzzy_score
    precision = total_matches / len(predicted) if predicted else 0
    recall = total_matches / len(ground_truth) if ground_truth else 0
    
    if precision + recall == 0:
        f1_score = 0.0
    else:
        f1_score = 2 * (precision * recall) / (precision + recall)
    
    return {
        "f1_score": round(f1_score, 3),
        "precision": round(precision, 3),
        "recall": round(recall, 3),
        "exact_matches": exact_matches,
        "fuzzy_matches": fuzzy_matches,
        "unmatched_predicted": final_unmatched_pred,
        "unmatched_ground_truth": final_unmatched_truth
    }

def calculate_match_score(got_count, expert_count):
    """ç®€å•çš„æ•°é‡æ¯”è¾ƒåˆ†æ•°"""
    if got_count == 0 and expert_count == 0:
        return 1.0
    if got_count == 0 or expert_count == 0:
        return 0.0
    return min(got_count, expert_count) / max(got_count, expert_count)

def process_directory(directory_path, verbose: bool = False):
    got_file_path = os.path.join(directory_path, 'got_selection_result.json')
    expert_file_path = os.path.join(directory_path, 'functions_cleaned.json')

    if not (os.path.exists(got_file_path) and os.path.exists(expert_file_path)):
        return

    try:
        # Read GOT results
        with open(got_file_path, 'r', encoding='utf-8') as f:
            got_data = json.load(f)
            got_ilf_list = got_data.get('ILF', [])
            got_eif_list = got_data.get('EIF', [])

        # Read Expert results
        with open(expert_file_path, 'r', encoding='utf-8') as f:
            expert_data = json.load(f)
            expert_ilf_list = []
            expert_eif_list = []
            for item in expert_data:
                f_type = item.get('functionType')
                f_name = item.get('functionName', '')
                if f_type == 'ILF':
                    expert_ilf_list.append(f_name)
                elif f_type == 'EIF':
                    expert_eif_list.append(f_name)

        print(f"\nğŸ“ å¤„ç†: {os.path.basename(os.path.dirname(directory_path))}/{os.path.basename(directory_path)}")
        
        # Calculate semantic similarity
        if verbose:
            print("  ğŸ“Š ILFæ¯”è¾ƒ:")
        ilf_similarity = calculate_semantic_similarity(got_ilf_list, expert_ilf_list, verbose=verbose)
        
        if verbose:
            print("  ğŸ“Š EIFæ¯”è¾ƒ:")
        eif_similarity = calculate_semantic_similarity(got_eif_list, expert_eif_list, verbose=verbose)

        # Calculate simple count-based scores
        ilf_count_score = calculate_match_score(len(got_ilf_list), len(expert_ilf_list))
        eif_count_score = calculate_match_score(len(got_eif_list), len(expert_eif_list))

        result = {
            "summary": {
                "got_ILF_count": len(got_ilf_list),
                "expert_ILF_count": len(expert_ilf_list),
                "got_EIF_count": len(got_eif_list),
                "expert_EIF_count": len(expert_eif_list),
                "ilf_count_match_score": round(ilf_count_score, 2),
                "eif_count_match_score": round(eif_count_score, 2),
                "ilf_semantic_f1": ilf_similarity["f1_score"],
                "eif_semantic_f1": eif_similarity["f1_score"],
                "use_llm_semantic": _USE_LLM_SEMANTIC
            },
            "ILF_comparison": {
                "got_list": got_ilf_list,
                "expert_list": expert_ilf_list,
                "semantic_metrics": {
                    "f1_score": ilf_similarity["f1_score"],
                    "precision": ilf_similarity["precision"],
                    "recall": ilf_similarity["recall"]
                },
                "exact_matches": ilf_similarity["exact_matches"],
                "fuzzy_matches": ilf_similarity["fuzzy_matches"],
                "unmatched_predicted": ilf_similarity["unmatched_predicted"],
                "unmatched_ground_truth": ilf_similarity["unmatched_ground_truth"]
            },
            "EIF_comparison": {
                "got_list": got_eif_list,
                "expert_list": expert_eif_list,
                "semantic_metrics": {
                    "f1_score": eif_similarity["f1_score"],
                    "precision": eif_similarity["precision"],
                    "recall": eif_similarity["recall"]
                },
                "exact_matches": eif_similarity["exact_matches"],
                "fuzzy_matches": eif_similarity["fuzzy_matches"],
                "unmatched_predicted": eif_similarity["unmatched_predicted"],
                "unmatched_ground_truth": eif_similarity["unmatched_ground_truth"]
            }
        }

        # Write result
        result_file_path = os.path.join(directory_path, 'comparison_result.json')
        with open(result_file_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=4, ensure_ascii=False)
        
        # Print summary
        print(f"   ILF: GOT={len(got_ilf_list)}, Expert={len(expert_ilf_list)}, F1={ilf_similarity['f1_score']:.3f} (P={ilf_similarity['precision']:.2f}, R={ilf_similarity['recall']:.2f})")
        print(f"   EIF: GOT={len(got_eif_list)}, Expert={len(expert_eif_list)}, F1={eif_similarity['f1_score']:.3f} (P={eif_similarity['precision']:.2f}, R={eif_similarity['recall']:.2f})")
        
        # Print match details
        if ilf_similarity["exact_matches"]:
            print(f"   ILFç²¾ç¡®åŒ¹é…({len(ilf_similarity['exact_matches'])}): {[m['predicted'] for m in ilf_similarity['exact_matches']]}")
        if ilf_similarity["fuzzy_matches"]:
            ilf_fuzzy_strs = [f"{m['predicted']} â†” {m['ground_truth']} ({m['score']})" for m in ilf_similarity['fuzzy_matches']]
            print(f"   ILFè¯­ä¹‰åŒ¹é…({len(ilf_similarity['fuzzy_matches'])}): {ilf_fuzzy_strs}")
        if ilf_similarity["unmatched_predicted"]:
            print(f"   ILFæœªåŒ¹é…(GOT): {ilf_similarity['unmatched_predicted']}")
        if ilf_similarity["unmatched_ground_truth"]:
            print(f"   ILFæœªåŒ¹é…(Expert): {ilf_similarity['unmatched_ground_truth']}")
            
        if eif_similarity["exact_matches"]:
            print(f"   EIFç²¾ç¡®åŒ¹é…({len(eif_similarity['exact_matches'])}): {[m['predicted'] for m in eif_similarity['exact_matches']]}")
        if eif_similarity["fuzzy_matches"]:
            eif_fuzzy_strs = [f"{m['predicted']} â†” {m['ground_truth']} ({m['score']})" for m in eif_similarity['fuzzy_matches']]
            print(f"   EIFè¯­ä¹‰åŒ¹é…({len(eif_similarity['fuzzy_matches'])}): {eif_fuzzy_strs}")
        if eif_similarity["unmatched_predicted"]:
            print(f"   EIFæœªåŒ¹é…(GOT): {eif_similarity['unmatched_predicted']}")
        if eif_similarity["unmatched_ground_truth"]:
            print(f"   EIFæœªåŒ¹é…(Expert): {eif_similarity['unmatched_ground_truth']}")

    except Exception as e:
        print(f"âŒ Error processing {directory_path}: {e}")
        import traceback
        traceback.print_exc()

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='GOT vs Expert åŠŸèƒ½ç‚¹è¯­ä¹‰æ¯”è¾ƒ')
    parser.add_argument('--use-llm', action='store_true', help='ä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰æ¯”è¾ƒï¼ˆæ›´ç²¾ç¡®ä½†æœ‰APIæˆæœ¬ï¼‰')
    parser.add_argument('--model', type=str, default='deepseek', help='LLMæ¨¡å‹åç§°')
    parser.add_argument('--verbose', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†çš„åŒ¹é…è¿‡ç¨‹')
    args = parser.parse_args()
    
    base_dir = os.path.join(os.path.dirname(__file__), 'requirement fetch')
    
    print("=" * 70)
    print("ğŸ” GOT vs Expert åŠŸèƒ½ç‚¹è¯­ä¹‰æ¯”è¾ƒ")
    print("=" * 70)
    
    # åˆå§‹åŒ–LLMï¼ˆå¦‚æœå¯ç”¨ï¼‰
    init_llm(model_name=args.model, use_semantic=args.use_llm)
    
    processed_count = 0
    
    # Walk through the directory
    for root, dirs, files in os.walk(base_dir):
        if 'got_selection_result.json' in files and 'functions_cleaned.json' in files:
            process_directory(root, verbose=args.verbose)
            processed_count += 1
    
    print("\n" + "=" * 70)
    print(f"âœ… å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {processed_count} ä¸ªç›®å½•")
    if _USE_LLM_SEMANTIC and _LLM_INSTANCE:
        print(f"ğŸ’° LLM APIæˆæœ¬: ${_LLM_INSTANCE.cost:.4f}")
    print("=" * 70)

if __name__ == "__main__":
    main()
