#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åŠŸèƒ½ç‚¹è¯„ä¼°å®éªŒå®ç°ã€‚
ä½¿ç”¨ä¸åŒçš„æç¤ºå·¥ç¨‹æ–¹æ³•ï¼ˆIOã€COTã€TOTã€GOTï¼‰æ¥ä»éœ€æ±‚æ–‡æ¡£ä¸­é€‰æ‹©å’Œè¯†åˆ«EIFåŠŸèƒ½ç‚¹ã€‚
"""

import os
import logging
import datetime
import json
import csv
import re
from functools import partial, total_ordering
from typing import Dict, List, Callable, Union
from graph_of_thoughts import controller, language_models, operations, prompter, parser

# å…¨å±€é…ç½®ï¼šç”¨äºè¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤æ–­çš„LLMå®ä¾‹
_GLOBAL_LM_FOR_SCORING = None
_USE_LLM_SEMANTIC = False  # é»˜è®¤å…³é—­ï¼ˆé¿å…é¢å¤–æˆæœ¬ï¼‰

def set_scoring_lm(lm, use_semantic: bool = False):
    """
    è®¾ç½®ç”¨äºè¯„åˆ†çš„LLMå®ä¾‹ã€‚
    
    :param lm: è¯­è¨€æ¨¡å‹å®ä¾‹
    :type lm: language_models.AbstractLanguageModel
    :param use_semantic: æ˜¯å¦å¯ç”¨LLMè¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤æ–­
    :type use_semantic: bool
    """
    global _GLOBAL_LM_FOR_SCORING, _USE_LLM_SEMANTIC
    _GLOBAL_LM_FOR_SCORING = lm
    _USE_LLM_SEMANTIC = use_semantic
    logging.info(f"Scoring LLM set, semantic matching: {use_semantic}")

def normalize_eif_name(name: str) -> str:
    """
    æ ‡å‡†åŒ–EIFåŠŸèƒ½ç‚¹åç§°ï¼Œç”¨äºæ¯”è¾ƒã€‚
    
    :param name: EIFåŠŸèƒ½ç‚¹åç§°
    :type name: str
    :return: æ ‡å‡†åŒ–åçš„åç§°
    :rtype: str
    """
    # è½¬å°å†™
    name = name.lower().strip()
    # å»é™¤å¤šä½™ç©ºæ ¼
    name = ' '.join(name.split())
    # å»é™¤æ‹¬å·å†…å®¹
    name = re.sub(r'\([^)]*\)', '', name)
    name = re.sub(r'ï¼ˆ[^ï¼‰]*ï¼‰', '', name)
    return name.strip()

def check_eif_semantic_similarity(name1: str, name2: str, lm=None, use_lm: bool = True) -> float:
    """
    ä½¿ç”¨LLMåˆ¤æ–­ä¸¤ä¸ªEIFåŠŸèƒ½ç‚¹åç§°æ˜¯å¦è¯­ä¹‰ç›¸åŒã€‚
    
    :param name1: ç¬¬ä¸€ä¸ªåŠŸèƒ½ç‚¹åç§°
    :type name1: str
    :param name2: ç¬¬äºŒä¸ªåŠŸèƒ½ç‚¹åç§°
    :type name2: str
    :param lm: è¯­è¨€æ¨¡å‹å®ä¾‹ï¼ˆå¯é€‰ï¼‰
    :type lm: language_models.AbstractLanguageModel
    :param use_lm: æ˜¯å¦ä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰åˆ¤æ–­
    :type use_lm: bool
    :return: ç›¸ä¼¼åº¦åˆ†æ•° (0.0 - 1.0)
    :rtype: float
    """
    # å¦‚æœä¸ä½¿ç”¨LLMæˆ–LLMæœªæä¾›ï¼Œå›é€€åˆ°å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
    if not use_lm or lm is None:
        return _string_similarity(name1, name2)
    
    # ä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤æ–­
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªIFPUGåŠŸèƒ½ç‚¹åˆ†æä¸“å®¶ã€‚è¯·åˆ¤æ–­ä»¥ä¸‹ä¸¤ä¸ªEIFï¼ˆå¤–éƒ¨æ¥å£æ–‡ä»¶ï¼‰åŠŸèƒ½ç‚¹åç§°æ˜¯å¦æŒ‡ä»£åŒä¸€ä¸ªåŠŸèƒ½ç‚¹ã€‚

åŠŸèƒ½ç‚¹1: {name1}
åŠŸèƒ½ç‚¹2: {name2}

è¯·åˆ†æï¼š
1. å®ƒä»¬æ˜¯å¦æŒ‡ä»£ç›¸åŒçš„å¤–éƒ¨æ•°æ®æºæˆ–æ¥å£ï¼Ÿ
2. è€ƒè™‘ä¸­è‹±æ–‡ç¿»è¯‘ã€åŒä¹‰è¯ã€ç¼©å†™ç­‰å› ç´ 
3. åªè¦è¯­ä¹‰ç›¸åŒå³å¯ï¼Œä¸éœ€è¦å®Œå…¨å­—é¢åŒ¹é…

è¯·ç›´æ¥å›ç­”ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0.0åˆ°1.0ä¹‹é—´çš„å°æ•°ï¼‰ï¼š
- 1.0: å®Œå…¨ç›¸åŒçš„åŠŸèƒ½ç‚¹
- 0.8-0.9: é«˜åº¦ç›¸ä¼¼ï¼Œå¾ˆå¯èƒ½æ˜¯åŒä¸€ä¸ªåŠŸèƒ½ç‚¹
- 0.5-0.7: ä¸­ç­‰ç›¸ä¼¼ï¼Œå¯èƒ½ç›¸å…³
- 0.0-0.4: ä¸ç›¸ä¼¼æˆ–ä¸ç›¸å…³

åªéœ€è¦å›ç­”ä¸€ä¸ªæ•°å­—ï¼Œæ ¼å¼ï¼š0.95"""

    try:
        # ä½¿ç”¨queryæ–¹æ³•è°ƒç”¨LLM
        query_response = lm.query(prompt, num_responses=1)
        # æå–å“åº”æ–‡æœ¬
        response_texts = lm.get_response_texts(query_response)
        
        if response_texts and len(response_texts) > 0:
            # æå–æ•°å­—
            text = response_texts[0].strip()
            logging.debug(f"LLM response for similarity check: '{text}'")
            match = re.search(r'(\d+\.?\d*)', text)
            if match:
                score = float(match.group(1))
                # ç¡®ä¿åœ¨0-1èŒƒå›´å†…
                score = max(0.0, min(1.0, score))
                logging.debug(f"LLM semantic similarity for '{name1}' vs '{name2}': {score}")
                return score
            else:
                logging.warning(f"Could not extract score from LLM response: '{text}'")
        else:
            logging.warning(f"Empty response from LLM for similarity check")
    except Exception as e:
        logging.warning(f"Error using LLM for semantic similarity: {e}")
    
    # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ°å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
    fallback_score = _string_similarity(name1, name2)
    logging.warning(f"Falling back to string similarity for '{name1}' vs '{name2}': {fallback_score:.2f}")
    return fallback_score

def calculate_eif_similarity(predicted: List[str], ground_truth: List[str], lm=None, use_lm_semantic: bool = True) -> Dict:
    """
    è®¡ç®—é¢„æµ‹çš„EIFåŠŸèƒ½ç‚¹åˆ—è¡¨å’ŒçœŸå®ç­”æ¡ˆçš„ç›¸ä¼¼åº¦ã€‚
    ä½¿ç”¨ç²¾ç¡®åŒ¹é…ã€è¯­ä¹‰åŒ¹é…ï¼ˆå¯é€‰LLMï¼‰ç›¸ç»“åˆçš„æ–¹æ³•ã€‚
    
    :param predicted: é¢„æµ‹çš„EIFåŠŸèƒ½ç‚¹åˆ—è¡¨
    :type predicted: List[str]
    :param ground_truth: çœŸå®çš„EIFåŠŸèƒ½ç‚¹åˆ—è¡¨
    :type ground_truth: List[str]
    :param lm: è¯­è¨€æ¨¡å‹å®ä¾‹ï¼ˆå¯é€‰ï¼Œç”¨äºè¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤æ–­ï¼‰
    :type lm: language_models.AbstractLanguageModel
    :param use_lm_semantic: æ˜¯å¦ä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤æ–­
    :type use_lm_semantic: bool
    :return: åŒ…å«ç›¸ä¼¼åº¦åˆ†æ•°å’Œè¯¦ç»†æŒ‡æ ‡çš„å­—å…¸ï¼ŒåŒ…æ‹¬ f1_score, precision, recall, exact_matches, fuzzy_score, semantic_matches
    :rtype: Dict
    """
    if not ground_truth and not predicted:
        # éƒ½ä¸ºç©ºï¼Œå®Œå…¨åŒ¹é…
        return {
            "f1_score": 1.0,
            "precision": 1.0,
            "recall": 1.0,
            "exact_matches": 0,
            "fuzzy_score": 0.0,
            "semantic_matches": []
        }
    if not ground_truth or not predicted:
        # ä¸€ä¸ªä¸ºç©ºï¼Œä¸€ä¸ªä¸ä¸ºç©º
        return {
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0,
            "exact_matches": 0,
            "fuzzy_score": 0.0,
            "semantic_matches": []
        }
    
    # æ ‡å‡†åŒ–æ‰€æœ‰åç§°
    pred_normalized = [normalize_eif_name(p) for p in predicted]
    truth_normalized = [normalize_eif_name(t) for t in ground_truth]
    
    # ç²¾ç¡®åŒ¹é…
    pred_set = set(pred_normalized)
    truth_set = set(truth_normalized)
    
    exact_matches = len(pred_set & truth_set)
    
    # è¯­ä¹‰/æ¨¡ç³ŠåŒ¹é…ï¼šå¯¹äºæ²¡æœ‰ç²¾ç¡®åŒ¹é…çš„é¡¹
    unmatched_pred = [(p, predicted[i]) for i, p in enumerate(pred_normalized) if p not in truth_set]
    unmatched_truth = [(t, ground_truth[i]) for i, t in enumerate(truth_normalized) if t not in pred_set]
    
    fuzzy_score = 0.0
    matched_truth = set()  # å­˜å‚¨å·²åŒ¹é…çš„åŸå§‹åç§°ï¼ˆè€Œéæ ‡å‡†åŒ–åç§°ï¼‰
    match_details = []
    
    for pred_norm, pred_orig in unmatched_pred:
        max_similarity = 0.0
        best_match = None
        best_match_orig = None
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        logging.debug(f"  Matching '{pred_orig}' against ground truth...")
        
        for truth_norm, truth_orig in unmatched_truth:
            # ä½¿ç”¨åŸå§‹åç§°åˆ¤æ–­æ˜¯å¦å·²åŒ¹é…ï¼ˆé¿å…æ ‡å‡†åŒ–ååç§°é‡å¤çš„é—®é¢˜ï¼‰
            if truth_orig in matched_truth:
                continue
            
            # é¦–å…ˆå°è¯•ä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤æ–­ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if use_lm_semantic and lm is not None:
                similarity = check_eif_semantic_similarity(pred_orig, truth_orig, lm, use_lm=True)
                logging.debug(f"    LLM similarity with '{truth_orig}': {similarity:.2f}")
            else:
                # å›é€€åˆ°å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
                similarity = _string_similarity(pred_norm, truth_norm)
                logging.debug(f"    String similarity with '{truth_orig}': {similarity:.2f}")
            
            if similarity > max_similarity:
                max_similarity = similarity
                best_match = truth_norm
                best_match_orig = truth_orig
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        logging.debug(f"  Best match for '{pred_orig}': '{best_match_orig}' (score: {max_similarity:.2f})")
        
        # å¦‚æœç›¸ä¼¼åº¦å¤§äºé˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯éƒ¨åˆ†åŒ¹é…
        if max_similarity > 0.7 and best_match:  # æé«˜é˜ˆå€¼åˆ°0.7ï¼ˆLLMæ›´å‡†ç¡®ï¼‰
            fuzzy_score += max_similarity
            matched_truth.add(best_match_orig)  # ä½¿ç”¨åŸå§‹åç§°è€Œéæ ‡å‡†åŒ–åç§°
            match_details.append(f"{pred_orig} <-> {best_match_orig} ({max_similarity:.2f})")
            logging.info(f"  âœ“ Matched: {pred_orig} <-> {best_match_orig} ({max_similarity:.2f})")
        else:
            logging.warning(f"  âœ— No match for '{pred_orig}' (best score: {max_similarity:.2f}, threshold: 0.7)")
    
    # æ€»åˆ† = ç²¾ç¡®åŒ¹é…åˆ†æ•° + æ¨¡ç³ŠåŒ¹é…åˆ†æ•°
    total_matches = exact_matches + fuzzy_score
    
    # ä½¿ç”¨F1-scoreçš„æ€æƒ³ï¼šåŒæ—¶è€ƒè™‘å¬å›ç‡å’Œå‡†ç¡®ç‡
    precision = total_matches / len(predicted) if predicted else 0
    recall = total_matches / len(ground_truth) if ground_truth else 0
    
    if precision + recall == 0:
        f1_score = 0.0
    else:
        f1_score = 2 * (precision * recall) / (precision + recall)
    
    logging.info(f"EIF Similarity - Predicted: {predicted}, Truth: {ground_truth}")
    logging.info(f"  Exact matches: {exact_matches}, Fuzzy score: {fuzzy_score:.2f}")
    if match_details:
        logging.info(f"  Semantic matches: {', '.join(match_details)}")
    logging.info(f"  Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1_score:.2f}")
    
    # è¿”å›è¯¦ç»†çš„è¯„ä¼°ç»“æœ
    return {
        "f1_score": f1_score,
        "precision": precision,
        "recall": recall,
        "exact_matches": exact_matches,
        "fuzzy_score": fuzzy_score,
        "semantic_matches": match_details
    }

def _string_similarity(s1: str, s2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ï¼ˆåŸºäºæœ€é•¿å…¬å…±å­åºåˆ—ï¼‰ã€‚
    
    :param s1: å­—ç¬¦ä¸²1
    :type s1: str
    :param s2: å­—ç¬¦ä¸²2
    :type s2: str
    :return: ç›¸ä¼¼åº¦ (0.0 - 1.0)
    :rtype: float
    """
    if not s1 or not s2:
        return 0.0
    
    # ç®€å•çš„åŸºäºé›†åˆçš„ç›¸ä¼¼åº¦ï¼ˆJaccardï¼‰
    set1 = set(s1.split())
    set2 = set(s2.split())
    
    if not set1 or not set2:
        # å¦‚æœæ²¡æœ‰ç©ºæ ¼åˆ†éš”ï¼Œä½¿ç”¨å­—ç¬¦çº§åˆ«
        set1 = set(s1)
        set2 = set(s2)
    
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    
    return intersection / union if union > 0 else 0.0

def test_eif_assessment(state: Dict) -> bool:
    """
    Function to test whether the final solution matches ground truth.
    æ”¯æŒä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤æ–­ã€‚

    :param state: Thought state that represents the final solution.
    :type state: Dict
    :return: Returns whether the solution matches the ground truth.
    :rtype: bool
    """
    try:
        # è·å–é¢„æµ‹çš„EIFåŠŸèƒ½ç‚¹åˆ—è¡¨
        if "final_answer" in state:
            prediction = state["final_answer"]  # ç°åœ¨æ˜¯åˆ—è¡¨
        else:
            # å‘åå…¼å®¹
            prediction = []
        
        ground_truth = state["ground_truth"]  # ç°åœ¨ä¹Ÿæ˜¯åˆ—è¡¨
        
        # ä½¿ç”¨å…¨å±€LLMè¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤æ–­
        similarity_result = calculate_eif_similarity(
            prediction, ground_truth,
            lm=_GLOBAL_LM_FOR_SCORING,
            use_lm_semantic=_USE_LLM_SEMANTIC
        )
        
        # ä¿å­˜è¯¦ç»†æŒ‡æ ‡åˆ°state
        state["evaluation_metrics"] = similarity_result
        
        return similarity_result["f1_score"] >= 0.8
    except Exception as e:
        logging.error(f"Error in test_eif_assessment: {e}")
        return False

def score_assessment(state: Dict) -> float:
    """
    Function to locally score the assessment that serves as a score.
    Inference mode: Returns 1.0 to bypass ground truth check.
    """
    return 1.0

class FunctionPointPrompter(prompter.Prompter):
    """
    FunctionPointPrompter provides the generation of prompts specific to the
    function point assessment example for the language models.

    Inherits from the Prompter class and implements its abstract methods.
    """

    ei_got_prompt = """ä½ æ˜¯ä¸€ä¸ªIFPUGåŠŸèƒ½ç‚¹åˆ†æä¸“å®¶ã€‚è¯·åˆ†æç»™å®šçš„éœ€æ±‚æ–‡æ¡£ï¼Œè¯†åˆ«å‡ºå…¶ä¸­æ‰€æœ‰å¯èƒ½çš„å¤–éƒ¨è¾“å…¥ï¼ˆEIï¼‰åŠŸèƒ½ç‚¹ã€‚

[éœ€æ±‚æ–‡æ¡£]
{requirement_text}

è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤åˆ†æï¼š

1. éœ€æ±‚åˆ†è§£
- è¯†åˆ«æ–‡æ¡£ä¸­æ‰€æœ‰è¾“å…¥æ•°æ®çš„åœºæ™¯
- åˆ†ææ•°æ®æ˜¯å¦è·¨è¶Šåº”ç”¨è¾¹ç•Œè¿›å…¥ç³»ç»Ÿ
- åˆ¤æ–­æ•°æ®æ˜¯ç”¨äºç»´æŠ¤ILFè¿˜æ˜¯æä¾›æ§åˆ¶ä¿¡æ¯

2. å¤šè·¯å¾„éªŒè¯
è·¯å¾„1ï¼šä»ç”¨æˆ·è§†è§’
- å“ªäº›ä¸šåŠ¡æ“ä½œæ¶‰åŠå‘ç³»ç»Ÿè¾“å…¥æ•°æ®
- å“ªäº›è¾“å…¥å¯¹ä¸šåŠ¡æœ‰ç‹¬ç«‹çš„æ„ä¹‰
- ç”¨æˆ·æ˜¯å¦å¯ä»¥è¯†åˆ«è¿™äº›è¾“å…¥

è·¯å¾„2ï¼šä»ç³»ç»Ÿè§†è§’
- å“ªäº›è¾“å…¥ä¼šè§¦å‘åå°å¤„ç†
- å“ªäº›è¾“å…¥ä¼šæ›´æ–°å†…éƒ¨æ•°æ®ï¼ˆILFï¼‰
- å“ªäº›è¾“å…¥æä¾›äº†æ§åˆ¶æŒ‡ä»¤

è·¯å¾„3ï¼šä»IFPUGè§„åˆ™è§†è§’
- æ£€æŸ¥æ˜¯å¦æ»¡è¶³EIå®šä¹‰ï¼ˆæ•°æ®ç©¿è¶Šè¾¹ç•Œè¿›å…¥ã€ç»´æŠ¤ILFæˆ–æ§åˆ¶ï¼‰
- æ’é™¤é‡å¤çš„æˆ–ä¸å®Œæ•´çš„è¾“å…¥
- ç¡®ä¿è¾“å…¥æ˜¯åŸºæœ¬å¤„ç†è¿‡ç¨‹

3. ç»“æœåˆå¹¶
- ç»¼åˆå„è·¯å¾„ç»“æœ
- å»é™¤é‡å¤å’Œä¸ç¬¦åˆæ¡ä»¶çš„
- å¾—å‡ºæœ€ç»ˆçš„EIåŠŸèƒ½ç‚¹åˆ—è¡¨

**ã€é‡è¦ã€‘è¯·åœ¨æœ€åä¸€è¡Œä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆå¿…é¡»ä½¿ç”¨æ–¹æ‹¬å·ï¼ŒåŠŸèƒ½ç‚¹ä¹‹é—´ç”¨é€—å·åˆ†éš”ï¼‰ï¼š**

EIåŠŸèƒ½ç‚¹åˆ—è¡¨ï¼š[åŠŸèƒ½ç‚¹1, åŠŸèƒ½ç‚¹2, åŠŸèƒ½ç‚¹3]

å¦‚æœæ²¡æœ‰EIåŠŸèƒ½ç‚¹ï¼Œè¯·è¾“å‡ºï¼š
EIåŠŸèƒ½ç‚¹åˆ—è¡¨ï¼šæ— """

    eo_got_prompt = """ä½ æ˜¯ä¸€ä¸ªIFPUGåŠŸèƒ½ç‚¹åˆ†æä¸“å®¶ã€‚è¯·åˆ†æç»™å®šçš„éœ€æ±‚æ–‡æ¡£ï¼Œè¯†åˆ«å‡ºå…¶ä¸­æ‰€æœ‰å¯èƒ½çš„å¤–éƒ¨è¾“å‡ºï¼ˆEOï¼‰åŠŸèƒ½ç‚¹ã€‚

[éœ€æ±‚æ–‡æ¡£]
{requirement_text}

è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤åˆ†æï¼š

1. éœ€æ±‚åˆ†è§£
- è¯†åˆ«æ–‡æ¡£ä¸­æ‰€æœ‰è¾“å‡ºæ•°æ®çš„åœºæ™¯
- åˆ†ææ•°æ®æ˜¯å¦è·¨è¶Šåº”ç”¨è¾¹ç•Œç¦»å¼€ç³»ç»Ÿ
- åˆ¤æ–­è¾“å‡ºæ˜¯å¦ç»è¿‡è®¡ç®—ã€æ¨å¯¼æˆ–æ›´æ–°äº†ILF

2. å¤šè·¯å¾„éªŒè¯
è·¯å¾„1ï¼šä»ç”¨æˆ·è§†è§’
- å“ªäº›æŠ¥è¡¨ã€é€šçŸ¥æˆ–æŸ¥è¯¢ç»“æœåŒ…å«è®¡ç®—é€»è¾‘
- å“ªäº›è¾“å‡ºå¯¹ä¸šåŠ¡æœ‰ç‹¬ç«‹çš„æ„ä¹‰
- ç”¨æˆ·è¯†åˆ«åˆ°çš„è¾“å‡ºæœ‰å“ªäº›

è·¯å¾„2ï¼šä»ç³»ç»Ÿè§†è§’
- å“ªäº›è¾“å‡ºæ¶‰åŠæ•°å­¦è¿ç®—æˆ–å¯¼å‡ºæ•°æ®
- å“ªäº›è¾“å‡ºæ”¹å˜äº†ç³»ç»ŸçŠ¶æ€ï¼ˆæ›´æ–°ILFï¼‰
- åŒºåˆ†ç®€å•çš„æ£€ç´¢ï¼ˆEQï¼‰å’Œå¤æ‚çš„è¾“å‡ºï¼ˆEOï¼‰

è·¯å¾„3ï¼šä»IFPUGè§„åˆ™è§†è§’
- æ£€æŸ¥æ˜¯å¦æ»¡è¶³EOå®šä¹‰ï¼ˆæ•°æ®ç©¿è¶Šè¾¹ç•Œç¦»å¼€ã€åŒ…å«è®¡ç®—/æ¨å¯¼/ILFç»´æŠ¤ï¼‰
- æ’é™¤ç®€å•çš„ç›´æ¥æ£€ç´¢ï¼ˆEQï¼‰
- ç¡®ä¿è¾“å‡ºæ˜¯åŸºæœ¬å¤„ç†è¿‡ç¨‹

3. ç»“æœåˆå¹¶
- ç»¼åˆå„è·¯å¾„ç»“æœ
- å»é™¤é‡å¤å’Œä¸ç¬¦åˆæ¡ä»¶çš„
- å¾—å‡ºæœ€ç»ˆçš„EOåŠŸèƒ½ç‚¹åˆ—è¡¨

**ã€é‡è¦ã€‘è¯·åœ¨æœ€åä¸€è¡Œä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆå¿…é¡»ä½¿ç”¨æ–¹æ‹¬å·ï¼ŒåŠŸèƒ½ç‚¹ä¹‹é—´ç”¨é€—å·åˆ†éš”ï¼‰ï¼š**

EOåŠŸèƒ½ç‚¹åˆ—è¡¨ï¼š[åŠŸèƒ½ç‚¹1, åŠŸèƒ½ç‚¹2, åŠŸèƒ½ç‚¹3]

å¦‚æœæ²¡æœ‰EOåŠŸèƒ½ç‚¹ï¼Œè¯·è¾“å‡ºï¼š
EOåŠŸèƒ½ç‚¹åˆ—è¡¨ï¼šæ— """

    eq_got_prompt = """ä½ æ˜¯ä¸€ä¸ªIFPUGåŠŸèƒ½ç‚¹åˆ†æä¸“å®¶ã€‚è¯·åˆ†æç»™å®šçš„éœ€æ±‚æ–‡æ¡£ï¼Œè¯†åˆ«å‡ºå…¶ä¸­æ‰€æœ‰å¯èƒ½çš„å¤–éƒ¨æŸ¥è¯¢ï¼ˆEQï¼‰åŠŸèƒ½ç‚¹ã€‚

[éœ€æ±‚æ–‡æ¡£]
{requirement_text}

è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤åˆ†æï¼š

1. éœ€æ±‚åˆ†è§£
- è¯†åˆ«æ–‡æ¡£ä¸­æ‰€æœ‰æ•°æ®æ£€ç´¢/æŸ¥çœ‹çš„åœºæ™¯
- åˆ†ææ•°æ®æ˜¯å¦è·¨è¶Šåº”ç”¨è¾¹ç•Œç¦»å¼€ç³»ç»Ÿ
- ç¡®è®¤å¤„ç†é€»è¾‘æ˜¯å¦ä»…ä¸ºç®€å•çš„æ£€ç´¢ï¼ˆæ— å¤æ‚è®¡ç®—ï¼‰

2. å¤šè·¯å¾„éªŒè¯
è·¯å¾„1ï¼šä»ç”¨æˆ·è§†è§’
- å“ªäº›æŸ¥è¯¢æ“ä½œä»…ç”¨äºæŸ¥çœ‹æ•°æ®
- å“ªäº›æŸ¥è¯¢ç»“æœæ˜¯ç›´æ¥ä»ILF/EIFæå–çš„
- ç”¨æˆ·è¯†åˆ«åˆ°çš„æŸ¥è¯¢æœ‰å“ªäº›

è·¯å¾„2ï¼šä»ç³»ç»Ÿè§†è§’
- å“ªäº›è¾“å‡ºä¸æ¶‰åŠå†…éƒ¨æ•°æ®çš„ä¿®æ”¹
- å“ªäº›è¾“å‡ºä¸åŒ…å«å¤æ‚çš„æ•°å­¦è¿ç®—æˆ–æ´¾ç”Ÿæ•°æ®
- åŒºåˆ†EQï¼ˆç®€å•æ£€ç´¢ï¼‰å’ŒEOï¼ˆå¤æ‚è¾“å‡ºï¼‰

è·¯å¾„3ï¼šä»IFPUGè§„åˆ™è§†è§’
- æ£€æŸ¥æ˜¯å¦æ»¡è¶³EQå®šä¹‰ï¼ˆè¾“å…¥+è¾“å‡ºç»„åˆã€æ•°æ®æ£€ç´¢ã€ä¸æ›´æ–°ILFã€æ— æ´¾ç”Ÿæ•°æ®ï¼‰
- ç¡®ä¿æŸ¥è¯¢æ˜¯ç‹¬ç«‹ä¸”ç”¨æˆ·å¯è¯†åˆ«çš„

3. ç»“æœåˆå¹¶
- ç»¼åˆå„è·¯å¾„ç»“æœ
- å»é™¤é‡å¤å’Œä¸ç¬¦åˆæ¡ä»¶çš„
- å¾—å‡ºæœ€ç»ˆçš„EQåŠŸèƒ½ç‚¹åˆ—è¡¨

**ã€é‡è¦ã€‘è¯·åœ¨æœ€åä¸€è¡Œä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆå¿…é¡»ä½¿ç”¨æ–¹æ‹¬å·ï¼ŒåŠŸèƒ½ç‚¹ä¹‹é—´ç”¨é€—å·åˆ†éš”ï¼‰ï¼š**

EQåŠŸèƒ½ç‚¹åˆ—è¡¨ï¼š[åŠŸèƒ½ç‚¹1, åŠŸèƒ½ç‚¹2, åŠŸèƒ½ç‚¹3]

å¦‚æœæ²¡æœ‰EQåŠŸèƒ½ç‚¹ï¼Œè¯·è¾“å‡ºï¼š
EQåŠŸèƒ½ç‚¹åˆ—è¡¨ï¼šæ— """

    def generate_prompt(self, num_branches: int, current: str, method: str, **kwargs) -> str:
        """
        Generate a generate prompt for the language model.
        """
        assert num_branches == 1, "Branching should be done via multiple requests."
        
        logging.debug(f"Method: {method}")
        logging.debug(f"Current state: {kwargs}")
        
        function_type = kwargs.get("function_type", "EI") # Default to EI if not specified
        
        # Support for CoT/IO/ToT which don't use the 'got' prefix check in the original logic but rely on fallthrough or specific handling
        if method.startswith("got") or method.startswith("cot") or method.startswith("io") or method.startswith("tot"):
            if "phase" in kwargs and kwargs["phase"] == "merge":
                # Ensure merge prompt matches function type if needed, or use generic one
                # For simplicity, using one generic merge prompt but injecting function_type could be better
                # But here we will just return the specific GOT prompt for the type if not in specific sub-phases
                # Wait, merge needs the perspectives. Providing a generic merge logic for now.
                 return self.merge_prompt.format(
                    requirement_text=kwargs["requirement_text"],
                    user_perspective=kwargs.get("user_perspective", ""),
                    system_perspective=kwargs.get("system_perspective", ""),
                    ifpug_perspective=kwargs.get("ifpug_perspective", "")
                ).replace("EIF", function_type) # Simple string replacement to adapt prompt
            
            elif "phase" in kwargs and kwargs["phase"] == "analysis":
                 return self.perspective_prompt.format(
                    perspective=kwargs["perspective"],
                    requirement_text=kwargs["requirement_text"]
                ).replace("EIF", function_type)

            else:
                # Top level GOT prompt
                if function_type == "EI":
                    return self.ei_got_prompt.format(requirement_text=kwargs["requirement_text"])
                elif function_type == "EO":
                    return self.eo_got_prompt.format(requirement_text=kwargs["requirement_text"])
                elif function_type == "EQ":
                    return self.eq_got_prompt.format(requirement_text=kwargs["requirement_text"])
                else:
                    return self.got_prompt.format(requirement_text=kwargs["requirement_text"]) # Fallback to EIF/Original

        return ""

    def aggregation_prompt(self, state_dicts: List[Dict], **kwargs) -> str:
        """
        Generate an aggregation prompt for the language model.

        :param state_dicts: The thought states that should be aggregated.
        :type state_dicts: List[Dict]
        :param kwargs: Additional keyword arguments.
        :return: The aggregation prompt.
        :rtype: str
        """
        pass

    def improve_prompt(self, current: str, aggr1: str, aggr2: str, **kwargs) -> str:
        """
        Generate an improve prompt for the language model.

        :param current: Intermediate solution.
        :type current: str
        :param aggr1: Partially solution 1 before aggregation.
        :type aggr1: str
        :param aggr2: Partially solution 2 before aggregation.
        :type aggr2: str
        :param kwargs: Additional keyword arguments.
        :return: The improve prompt.
        :rtype: str
        """
        pass

    def validation_prompt(self, **kwargs) -> str:
        """
        Generate a validation prompt for the language model.

        :param kwargs: Additional keyword arguments.
        :return: The validation prompt.
        :rtype: str
        """
        pass

    def score_prompt(self, state_dicts: List[Dict], **kwargs) -> str:
        """
        Generate a score prompt for the language model.

        :param state_dicts: The thought states that should be scored,
                            if more than one, they should be scored together.
        :type state_dicts: List[Dict]
        :param kwargs: Additional keyword arguments.
        :return: The score prompt.
        :rtype: str
        """
        pass

class FunctionPointParser(parser.Parser):
    """
    FunctionPointParser provides the parsing of language model responses specific to the
    function point assessment example.

    Inherits from the Parser class and implements its abstract methods.
    """

    def extract_answer(self, text: str) -> List[str]:
        """
        ä»æ–‡æœ¬ä¸­æå–åŠŸèƒ½ç‚¹åˆ—è¡¨ (Generic for EI/EO/EQ/EIF/ILF).

        :param text: åŒ…å«ç­”æ¡ˆçš„æ–‡æœ¬
        :type text: str
        :return: æå–å‡ºçš„åŠŸèƒ½ç‚¹åˆ—è¡¨
        :rtype: List[str]
        """
        logging.info(f"Extracting Function Points from text (length: {len(text) if text else 0})")
        
        if not text:
            logging.warning("Empty text received for extraction")
            return []
        
        text = text.strip()
        
        # Generic patterns for diverse types
        final_patterns = [
            r'\*\*æœ€ç»ˆ[A-Z]*åŠŸèƒ½ç‚¹åˆ—è¡¨\*\*[ï¼š:]\s*\[([^\]]+)\]',
            r'\*\*æœ€ç»ˆ[A-Z]*åŠŸèƒ½ç‚¹åˆ—è¡¨\*\*[ï¼š:]\s*([^\n]+)',
            r'æœ€ç»ˆ[A-Z]*åŠŸèƒ½ç‚¹åˆ—è¡¨[ï¼š:]\s*\[([^\]]+)\]',
            r'æœ€ç»ˆ[A-Z]*åŠŸèƒ½ç‚¹åˆ—è¡¨[ï¼š:]\s*([^\n]+)',
            r'æœ€ç»ˆ.*?åŠŸèƒ½ç‚¹.*?åˆ—è¡¨[ï¼š:]\s*\[([^\]]+)\]',
            r'\*?\*?[A-Z]*åŠŸèƒ½ç‚¹åˆ—è¡¨\*?\*?[ï¼š:]\s*\[([^\]]+)\]',  
            r'[A-Z]*åŠŸèƒ½ç‚¹åˆ—è¡¨[ï¼š:]\s*([^\n]+)',
            r'[A-Z]*åŠŸèƒ½ç‚¹åˆ—è¡¨[ï¼š:]\s*\[([^\]]+)\]',
        ]

        if len(text) > 500:
             logging.info("Long text detected, searching for final conclusion markers")
             for pattern in final_patterns:
                match = re.search(pattern, text, re.DOTALL)
                if match:
                    fp_text = match.group(1).strip()
                    logging.info(f"Found final conclusion with pattern: {pattern}")
                    # Check for "None"
                    if fp_text in ["æ— ", "æ— ã€‚", "None", "none"]:
                        logging.info(f"Detected 'æ— ' in final conclusion, returning empty list")
                        return []
                    fp_list = [self._clean_name(item.strip()) for item in re.split(r'[,ï¼Œã€;ï¼›]', fp_text)]
                    fp_list = [item for item in fp_list if item and len(item) < 80] # Increased len limit slightly
                    if fp_list:
                         logging.info(f"Extracted FP from final conclusion: {fp_list}")
                         return fp_list

        
        # Fallback to patterns
        for pattern in final_patterns:
            match = re.search(pattern, text)
            if match:
                answer_text = match.group(1).strip()
                if answer_text in ["æ— ", "æ— ã€‚", "None", "none"]:
                     return []
                if answer_text in ["", "**", "*"]:
                     continue
                fp_list = re.split(r'[,ï¼Œã€;ï¼›]', answer_text)
                fp_list = [self._clean_name(item.strip()) for item in fp_list if item.strip()]
                if fp_list:
                    logging.info(f"Extracted FP list: {fp_list}")
                    return fp_list

        # Attempt numbered list extraction
        pattern_list = r'\d+\.\s*\*\*([A-Z][A-Za-z_\s]+)\*\*'
        matches = re.findall(pattern_list, text)
        if matches:
             fp_list = [self._clean_name(m.strip()) for m in matches]
             fp_list = [item for item in fp_list if item and 2 < len(item) < 80 and 'åˆ—è¡¨' not in item]
             if fp_list:
                 logging.info(f"Extracted FP from bold list: {fp_list}")
                 return fp_list
                 
        return []

    def _clean_name(self, name: str) -> str:
        """
        æ¸…ç†åŠŸèƒ½ç‚¹åç§°ã€‚
        """
        name = re.sub(r'^\d+[\.\)ã€]\s*', '', name)
        name = re.sub(r'^[-â€¢Â·]\s*', '', name)
        name = re.sub(r'[\[\]]', '', name)
        name = ' '.join(name.split())
        return name

    def parse_generate_answer(self, state: Dict, texts: List[str]) -> List[Dict]:
        """
        Parse the response from the language model for a generate prompt.

        :param state: The thought state used to generate the prompt.
        :type state: Dict
        :param texts: The responses to the prompt from the language model.
        :type texts: List[str]
        :return: The new thought states after parsing the responses from the language model.
        :rtype: List[Dict]
        """
        new_states = []
        for text in texts:
            try:
                new_state = state.copy()
                
                # ä¿å­˜åŸå§‹å›ç­”
                new_state["current"] = text
                
                # æå–EIFåŠŸèƒ½ç‚¹åˆ—è¡¨
                answer = self.extract_answer(text)
                new_state["final_answer"] = answer  # ç°åœ¨æ˜¯ä¸€ä¸ªåˆ—è¡¨
                
                # æ ¹æ®ä¸åŒé˜¶æ®µå­˜å‚¨åˆ†æç»“æœ
                if "perspective" in state:
                    # è§†è§’åˆ†æé˜¶æ®µ
                    perspective = state["perspective"]
                    new_state[f"{perspective}_analysis"] = text
                    # å°†åˆ†æç»“æœä¹Ÿå­˜å‚¨åˆ°åˆå¹¶é˜¶æ®µä¼šç”¨åˆ°çš„é”®ä¸­
                    if perspective == "ç”¨æˆ·è§†è§’":
                        new_state["user_perspective"] = text
                    elif perspective == "ç³»ç»Ÿè§†è§’":
                        new_state["system_perspective"] = text
                    elif perspective == "IFPUGè§„åˆ™è§†è§’":
                        new_state["ifpug_perspective"] = text
                elif "merge_perspectives" in state:
                    # åˆå¹¶é˜¶æ®µ
                    new_state["merged_analysis"] = text
                
                new_states.append(new_state)
            except Exception as e:
                logging.error(f"Could not parse answer: {text}. Error: {e}")
                # å‘ç”Ÿé”™è¯¯æ—¶æ·»åŠ ä¸€ä¸ªé»˜è®¤çŠ¶æ€
                default_state = state.copy()
                default_state["current"] = text
                default_state["final_answer"] = []  # é»˜è®¤ä¸ºç©ºåˆ—è¡¨
                default_state["parse_error"] = str(e)
                new_states.append(default_state)
        return new_states

    def parse_aggregation_answer(self, states: List[Dict], texts: List[str]) -> Union[Dict, List[Dict]]:
        """
        Parse the response from the language model for an aggregation prompt.

        :param states: The thought states used to generate the prompt.
        :type states: List[Dict]
        :param texts: The responses to the prompt from the language model.
        :type texts: List[str]
        :return: The new thought states after parsing the respones from the language model.
        :rtype: Union[Dict, List[Dict]]
        """
        pass

    def parse_improve_answer(self, state: Dict, texts: List[str]) -> Dict:
        """
        Parse the response from the language model for an improve prompt.

        :param state: The thought state used to generate the prompt.
        :type state: Dict
        :param texts: The responses to the prompt from the language model.
        :type texts: List[str]
        :return: The new thought state after parsing the responses from the language model.
        :rtype: Dict
        """
        pass

    def parse_validation_answer(self, state: Dict, texts: List[str]) -> bool:
        """
        Parse the response from the language model for a validation prompt.

        :param state: The thought state used to generate the prompt.
        :type state: Dict
        :param texts: The responses to the prompt from the language model.
        :type texts: List[str]
        :return: Whether the thought state is valid or not.
        :rtype: bool
        """
        pass

    def parse_score_answer(self, states: List[Dict], texts: List[str]) -> List[float]:
        """
        Parse the response from the language model for a score prompt.

        :param states: The thought states used to generate the prompt.
        :type states: List[Dict]
        :param texts: The responses to the prompt from the language model.
        :type texts: List[str]
        :return: The scores for the thought states.
        :rtype: List[float]
        """
        pass

def io() -> operations.GraphOfOperations:
    """
    Generates the Graph of Operations for the IO method.

    :return: Graph of Operations
    :rtype: GraphOfOperations
    """
    operations_graph = operations.GraphOfOperations()

    operations_graph.append_operation(operations.Generate(1, 1))
    operations_graph.append_operation(operations.Score(1, False, score_assessment))
    operations_graph.append_operation(operations.GroundTruth(test_eif_assessment))

    return operations_graph

def cot() -> operations.GraphOfOperations:
    """
    Generates the Graph of Operations for the CoT method.

    :return: Graph of Operations
    :rtype: GraphOfOperations
    """
    operations_graph = operations.GraphOfOperations()

    operations_graph.append_operation(operations.Generate(1, 1))
    operations_graph.append_operation(operations.Score(1, False, score_assessment))
    # operations_graph.append_operation(operations.GroundTruth(test_eif_assessment)) # Inference mode

    return operations_graph

def tot() -> operations.GraphOfOperations:
    """
    Generates the Graph of Operations for the ToT method.

    :return: Graph of Operations
    :rtype: GraphOfOperations
    """
    operations_graph = operations.GraphOfOperations()

    # ç›®å‰æœ‰é—®é¢˜ï¼Œdeepseekä¸æ”¯æŒè¯•ç”¨å‚æ•°nç”Ÿæˆå¤šä¸ªå›å¤choices
    operations_graph.append_operation(operations.Generate(1, 1))
    operations_graph.append_operation(operations.Score(1, False, score_assessment))
    keep_best_1 = operations.KeepBestN(1, True)  # True: é€‰æ‹©æœ€é«˜åˆ†æ•°
    operations_graph.append_operation(keep_best_1)

    for _ in range(3):
        operations_graph.append_operation(operations.Generate(1, 1))
        operations_graph.append_operation(operations.Score(1, False, score_assessment))
        keep_best_2 = operations.KeepBestN(1, True)  # True: é€‰æ‹©æœ€é«˜åˆ†æ•°
        keep_best_2.add_predecessor(keep_best_1)
        operations_graph.append_operation(keep_best_2)
        keep_best_1 = keep_best_2

    operations_graph.append_operation(operations.KeepBestN(1, True))  # True: é€‰æ‹©æœ€é«˜åˆ†æ•°
    operations_graph.append_operation(operations.GroundTruth(test_eif_assessment))

    return operations_graph

def got() -> operations.GraphOfOperations:
    """
    Generates the Graph of Operations for the GoT method.
    ä½¿ç”¨å›¾ç»“æ„æ¥åˆ†æEIFåˆ¤æ–­é—®é¢˜ï¼š
    1. ä»ä¸‰ä¸ªä¸åŒè§†è§’åˆ†æï¼ˆç”¨æˆ·è§†è§’ã€ç³»ç»Ÿè§†è§’ã€IFPUGè§„åˆ™è§†è§’ï¼‰
    2. æ¯ä¸ªè§†è§’ç”Ÿæˆå¤šä¸ªæ€è·¯å¹¶é€‰æ‹©æœ€ä½³
    3. åˆå¹¶å’ŒéªŒè¯ç»“æœ
    """
    operations_graph = operations.GraphOfOperations()

    # 1. ä»ä¸‰ä¸ªä¸åŒè§†è§’è¿›è¡Œåˆ†æ
    perspectives = ["ç”¨æˆ·è§†è§’", "ç³»ç»Ÿè§†è§’", "IFPUGè§„åˆ™è§†è§’"]
    perspective_results = []
    
    for perspective in perspectives:
        # 1.1 ç”Ÿæˆè¯¥è§†è§’çš„åˆ†æ
        generate = operations.Generate(1, 1)
        # å°†è§†è§’ä¿¡æ¯æ·»åŠ åˆ°åˆå§‹çŠ¶æ€ä¸­
        generate.initial_state = {
            "perspective": perspective,
            "phase": "analysis"
        }
        operations_graph.add_operation(generate)

        # 1.2 è¯„åˆ†
        score = operations.Score(1, False, score_assessment)
        score.add_predecessor(generate)
        operations_graph.add_operation(score)
        
        # 1.3 ä¿ç•™æœ€ä½³ç»“æœ
        keep_best = operations.KeepBestN(1, True)
        keep_best.add_predecessor(score)
        operations_graph.add_operation(keep_best)
        
        perspective_results.append(keep_best)

    # 2. åˆå¹¶ä¸‰ä¸ªè§†è§’çš„ç»“æœ
    merge = operations.Generate(1, 1)
    # è®¾ç½®åˆå¹¶é˜¶æ®µçš„çŠ¶æ€
    merge.initial_state = {
        "phase": "merge",
        "merge_perspectives": True
    }
    for result in perspective_results:
        merge.add_predecessor(result)
    operations_graph.add_operation(merge)

    # 3. è¯„åˆ†å’Œé€‰æ‹©æœ€ç»ˆç»“æœ
    final_score = operations.Score(1, False, score_assessment)
    final_score.add_predecessor(merge)
    operations_graph.add_operation(final_score)

    final_keep = operations.KeepBestN(1, True)
    final_keep.add_predecessor(final_score)
    operations_graph.add_operation(final_keep)

    # 4. éªŒè¯ - Inference mode: No ground truth check
    # operations_graph.append_operation(operations.GroundTruth(test_eif_assessment))

    return operations_graph

def got_ei() -> operations.GraphOfOperations: return got()
def got_eo() -> operations.GraphOfOperations: return got()
def got_eq() -> operations.GraphOfOperations: return got()

def cot_ei() -> operations.GraphOfOperations: return cot()
def cot_eo() -> operations.GraphOfOperations: return cot()
def cot_eq() -> operations.GraphOfOperations: return cot()

def run(data_ids: List[int], methods: List[Callable[[], operations.GraphOfOperations]], budget: float, lm_name: str) -> float:
    """
    Controller function that executes each specified method for each specified
    sample while the budget is not exhausted.

    :param data_ids: Indices of the sample to be run.
    :type data_ids: List[int]
    :param methods: List of functions to generate Graphs of Operations.
    :type methods: Each function generates a Graph of Operation.
    :param budget: Language model budget for the execution in dollars.
    :type budget: float
    :param lm_name: Name of the language model to be used.
    :type lm_name: str
    :return: Spent budget in dollars.
    :rtype: float
    """
    orig_budget = budget
    data_path = os.path.join(os.path.dirname(__file__), "eif_selection.csv")
    data = []
    with open(data_path, "r", encoding="gbk") as f:  # ä½¿ç”¨ GBK ç¼–ç ï¼ˆæ–‡ä»¶å®é™…ç¼–ç ï¼‰
        reader = csv.reader(f)
        next(reader)  # Skip header
        for row in reader:
            # æ–°æ ¼å¼: doc_id, true_eif, requirement_text
            # true_eif æ˜¯é€—å·åˆ†éš”çš„EIFåŠŸèƒ½ç‚¹åˆ—è¡¨
            doc_id = int(row[0])
            true_eif_str = row[1].strip()
            requirement_text = row[2]
            
            # å°†true_eifå­—ç¬¦ä¸²è§£æä¸ºåˆ—è¡¨
            if true_eif_str and true_eif_str.lower() not in ["æ— ", "none", ""]:
                true_eif_list = [item.strip() for item in re.split(r'[,ï¼Œ]', true_eif_str) if item.strip()]
            else:
                true_eif_list = []
            
            # æ³¨æ„ï¼šdataç»“æ„ä¸º [doc_id, true_eif_list, requirement_text]
            # è¿™æ ·data[1]å°±æ˜¯åŠŸèƒ½ç‚¹åˆ—è¡¨ï¼Œdata[2]å°±æ˜¯éœ€æ±‚æ–‡æ¡£
            data.append([doc_id, true_eif_list, requirement_text])

    if data_ids is None or len(data_ids) == 0:
        data_ids = list(range(len(data)))
    selected_data = [data[i] for i in data_ids]

    results_dir = os.path.join(os.path.dirname(__file__), "results")

    if not os.path.exists(results_dir):
        os.makedirs(results_dir)
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    extra_info = f"{lm_name}_{'-'.join([method.__name__ for method in methods])}"
    folder_name = f"{extra_info}_{timestamp}"
    results_folder = os.path.join(results_dir, folder_name)
    os.makedirs(results_folder)

    config = {
        "data": selected_data,
        "methods": [method.__name__ for method in methods],
        "lm": lm_name,
        "budget": budget,
    }
    with open(os.path.join(results_folder, "config.json"), "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)

    logging.basicConfig(
        filename=os.path.join(results_folder, "log.log"),
        filemode="w",
        format="%(name)s - %(levelname)s - %(message)s",
        level=logging.DEBUG,
        encoding="utf-8"
    )

    for method in methods:
        # create a results directory for the method
        os.makedirs(os.path.join(results_folder, method.__name__))

    for data in selected_data:
        logging.info(f"Running data {data[0]}: Ground truth EIF count {len(data[1])}, Requirement text length {len(data[2])}")
        if budget <= 0.0:
            logging.error(f"Budget has been depleted, stopping. Data {data[0]} has not been run.")
            break
        for method in methods:
            logging.info(f"Running method {method.__name__}")
            logging.info(f"Budget left: {budget}")
            if budget <= 0.0:
                logging.error(f"Budget has been depleted, stopping. Method {method.__name__} has not been run.")
                break

            lm = language_models.ChatGPT(
                os.path.join(
                    os.path.dirname(__file__),
                    "../../graph_of_thoughts/language_models/config.json",
                ),
                model_name=lm_name,
                cache=True,
            )
            
            # è®¾ç½®ç”¨äºè¯„åˆ†çš„LLMï¼ˆå¯é€‰å¯ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤æ–­ï¼‰
            # æ³¨æ„ï¼šå¯ç”¨ä¼šå¢åŠ APIè°ƒç”¨æˆæœ¬ï¼Œå»ºè®®åœ¨è¯„ä¼°é˜¶æ®µä½¿ç”¨
            use_semantic = True  # è®¾ç½®ä¸ºTrueå¯ç”¨LLMè¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤æ–­
            set_scoring_lm(lm, use_semantic=use_semantic)
            
            operations_graph = method()
            executor = controller.Controller(
                lm,
                operations_graph,
                FunctionPointPrompter(),
                FunctionPointParser(),
                {
                    "requirement_text": data[2],  # éœ€æ±‚æ–‡æ¡£ (data[1])
                    "ground_truth": data[1],  # EIFåŠŸèƒ½ç‚¹åˆ—è¡¨ (data[2])
                    "current": "",
                    "method": method.__name__,
                },
            )
            try:
                executor.run()
            except Exception as e:
                logging.error(f"Exception: {e}")
            path = os.path.join(
                results_folder,
                method.__name__,
                f"{data[0]}.json",
            )
            executor.output_graph(path)
            budget -= lm.cost

    return orig_budget - budget

if __name__ == "__main__":
    """
    Input (x)   : éœ€æ±‚æ–‡æ¡£
    Output (y)  : EIFåŠŸèƒ½ç‚¹åˆ—è¡¨
    Correct     : è®¡ç®—é¢„æµ‹åˆ—è¡¨å’ŒçœŸå®åˆ—è¡¨çš„ç›¸ä¼¼åº¦
    Input Example:
        éœ€æ±‚æ–‡æ¡£ï¼šäººåŠ›èµ„æºç®¡ç†ç³»ç»Ÿ - èŒä½ä¿¡æ¯ç®¡ç†æ¨¡å—...
    Output Example:
        [Job information, Employee information]
    """
    # è®¾ç½®æ§åˆ¶å°æ—¥å¿—è¾“å‡º
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),  # æ§åˆ¶å°è¾“å‡º
            logging.FileHandler('experiment.log', encoding='utf-8')  # æ–‡ä»¶è¾“å‡º
        ]
    )
    
    print("ğŸš€ å¼€å§‹è¿è¡ŒEIFåŠŸèƒ½ç‚¹åˆ†æå®éªŒ...")
    print("=" * 50)
    
    budget = 5
    samples = [0,1,2,3,4,5]  # ä½¿ç”¨å‰ä¸¤ä¸ªæ ·æœ¬è¿›è¡Œæµ‹è¯•
    approaches = [got]  # å…ˆç”¨ç®€å•æ–¹æ³•æµ‹è¯•

    print(f"ğŸ“Š å®éªŒé…ç½®:")
    print(f"   - é¢„ç®—: ${budget}")
    print(f"   - æ ·æœ¬æ•°é‡: {len(samples)}")
    print(f"   - æ–¹æ³•: {[method.__name__ for method in approaches]}")
    print(f"   - æ¨¡å‹: r1-7b")
    print("=" * 50)


    spent = run(samples, approaches, budget, "qwen3-30b")

    print("=" * 50)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         
    print(f"âœ… å®éªŒå®Œæˆï¼")
    print(f"ğŸ’° èŠ±è´¹: ${spent:.2f} / ${budget}")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: results/ ç›®å½•")
    logging.info(f"Spent {spent} out of {budget} budget.") 