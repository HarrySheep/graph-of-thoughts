#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ÂäüËÉΩÁÇπËØÑ‰º∞ÂÆûÈ™åÂÆûÁé∞„ÄÇ
‰ΩøÁî®‰∏çÂêåÁöÑÊèêÁ§∫Â∑•Á®ãÊñπÊ≥ïÔºàIO„ÄÅCOT„ÄÅTOT„ÄÅGOTÔºâÊù•Âà§Êñ≠ÂäüËÉΩÁÇπÊòØÂê¶‰∏∫EIF„ÄÇ
"""

import os
import logging
import datetime
import json
import csv
import re
from functools import partial, total_ordering
from typing import Dict, List, Callable, Union
from graph_of_thoughts import controller, language_models, operations, prompter, parser

def test_eif_assessment(state: Dict) -> bool:
    """
    Function to test whether the final solution matches ground truth.

    :param state: Thought state that represents the final solution.
    :type state: Dict
    :return: Returns whether the solution matches the ground truth.
    :rtype: bool
    """
    try:
        # ‰ºòÂÖà‰ΩøÁî®final_answerÔºåÁ°Æ‰øùÂÖ∂‰∏∫Â∏ÉÂ∞îÁ±ªÂûã
        if "final_answer" in state:
            prediction = state["final_answer"]  # Â∑≤ÁªèÊòØÂ∏ÉÂ∞îÁ±ªÂûã
        else:
            # ÂêëÂêéÂÖºÂÆπÔºåÂ§ÑÁêÜcurrent
            prediction = state["current"].lower().strip() == "ÊòØ"
        
        ground_truth = state["ground_truth"]  # Â∑≤ÁªèÊòØÂ∏ÉÂ∞îÁ±ªÂûã
        return prediction == ground_truth
    except:
        return False

def score_assessment(state: Dict) -> float:
    """
    Function to locally score the assessment that serves as a score.

    :param state: Thought state to be scored.
    :type state: Dict
    :return: Score (0 or 1).
    :rtype: float
    """
    try:
        # ‰ºòÂÖà‰ΩøÁî®final_answerÔºåÁ°Æ‰øùÂÖ∂‰∏∫Â∏ÉÂ∞îÁ±ªÂûã
        if "final_answer" in state:
            prediction = state["final_answer"]  # Â∑≤ÁªèÊòØÂ∏ÉÂ∞îÁ±ªÂûã
        else:
            # ÂêëÂêéÂÖºÂÆπÔºåÂ§ÑÁêÜcurrent
            prediction = state["current"].lower().strip() == "ÊòØ"
            
        ground_truth = state["ground_truth"]  # Â∑≤ÁªèÊòØÂ∏ÉÂ∞îÁ±ªÂûã
        return 1.0 if prediction == ground_truth else 0.0
    except:
        return 0.0

class FunctionPointPrompter(prompter.Prompter):
    """
    FunctionPointPrompter provides the generation of prompts specific to the
    function point assessment example for the language models.

    Inherits from the Prompter class and implements its abstract methods.
    """

    io_prompt = """‰Ω†ÊòØ‰∏Ä‰∏™IFPUGÂäüËÉΩÁÇπÂàÜÊûê‰∏ìÂÆ∂„ÄÇËØ∑Âà§Êñ≠ÁªôÂÆöÁöÑÂäüËÉΩÁÇπÊòØÂê¶ÊûÑÊàêÂ§ñÈÉ®Êé•Âè£Êñá‰ª∂ÔºàEIFÔºâ„ÄÇ
Âè™ÈúÄÂõûÁ≠î"ÊòØ"Êàñ"Âê¶"„ÄÇ

[ÈúÄÊ±ÇÊñáÊ°£]
{requirement_text}

[ÂÄôÈÄâÂäüËÉΩÁÇπ]
ÂêçÁß∞Ôºö{candidate_name}"""

    cot_prompt = """‰Ω†ÊòØ‰∏Ä‰∏™IFPUGÂäüËÉΩÁÇπÂàÜÊûê‰∏ìÂÆ∂„ÄÇËØ∑Âà§Êñ≠ÁªôÂÆöÁöÑÂäüËÉΩÁÇπÊòØÂê¶ÊûÑÊàêÂ§ñÈÉ®Êé•Âè£Êñá‰ª∂ÔºàEIFÔºâ„ÄÇ
ËØ∑ÊåâÁÖß‰ª•‰∏ãÊ≠•È™§ËøõË°åÂàÜÊûêÔºö

1. È¶ñÂÖàÔºåÂà§Êñ≠ÊòØÂê¶ÈÄªËæë‰∏äÁã¨Á´ã‰∏îÁî®Êà∑ÂèØËØÜÂà´
2. ÁÑ∂ÂêéÔºåÂà§Êñ≠ÊòØÂê¶Ë¢´ÂΩìÂâçÂ∫îÁî®ÂºïÁî®Ôºå‰ΩÜÁâ©ÁêÜ/ÈÄªËæë‰∏äÂ≠òÂú®‰∫éÂΩìÂâçÂ∫îÁî®‰πãÂ§ñ
3. ÊúÄÂêéÔºåÂà§Êñ≠ÊòØÂê¶‰∏çÁî±ÂΩìÂâçÂ∫îÁî®ËøõË°åÁª¥Êä§ÔºàÂç≥Âè™ËØªÔºå‰∏çÂ¢ûÂà†ÊîπÔºâ
4. Ê†πÊçÆ‰ª•‰∏äÂàÜÊûêÔºåÂæóÂá∫ÊúÄÁªàÁªìËÆ∫

[ÈúÄÊ±ÇÊñáÊ°£]
{requirement_text}

[ÂÄôÈÄâÂäüËÉΩÁÇπ]
ÂêçÁß∞Ôºö{candidate_name}

ËØ∑Êåâ‰ª•‰∏ãÊ†ºÂºèËæìÂá∫Ôºö
ÊÄùËÄÉËøáÁ®ãÔºö
1. [ÂàÜÊûêÁ¨¨‰∏Ä‰∏™Êù°‰ª∂]
2. [ÂàÜÊûêÁ¨¨‰∫å‰∏™Êù°‰ª∂]
3. [ÂàÜÊûêÁ¨¨‰∏â‰∏™Êù°‰ª∂]
4. [ÂæóÂá∫ÁªìËÆ∫]

ÊúÄÁªàÁ≠îÊ°àÔºö[ÊòØ/Âê¶]"""

    tot_prompt = """‰Ω†ÊòØ‰∏Ä‰∏™IFPUGÂäüËÉΩÁÇπÂàÜÊûê‰∏ìÂÆ∂„ÄÇËØ∑Âà§Êñ≠ÁªôÂÆöÁöÑÂäüËÉΩÁÇπÊòØÂê¶ÊûÑÊàêÂ§ñÈÉ®Êé•Âè£Êñá‰ª∂ÔºàEIFÔºâ„ÄÇ

[ÈúÄÊ±ÇÊñáÊ°£]
{requirement_text}

[ÂÄôÈÄâÂäüËÉΩÁÇπ]
ÂêçÁß∞Ôºö{candidate_name}

ËØ∑Êåâ‰ª•‰∏ãÊñπÊ≥ïÂàÜÊûêÂÄôÈÄâÂäüËÉΩÁÇπÊòØÂê¶‰∏∫EIFÂäüËÉΩÁÇπÔºö

1. ÂàùÊ≠•Âà§Êñ≠
   1.1 [Á¨¨‰∏ÄÂç∞Ë±°]
   1.2 [ÂèØËÉΩÁöÑÈóÆÈ¢ò]

2. Ê∑±ÂÖ•ÂàÜÊûê
   2.1 Êï∞ÊçÆÁªÑÁâπÂæÅ
       - [ÂàÜÊûêÊï∞ÊçÆÊòØÂê¶ÈÄªËæë‰∏äÁã¨Á´ã]
       - [ÂàÜÊûêÁî®Êà∑ÂèØËØÜÂà´ÊÄß]
   2.2 Êï∞ÊçÆ‰ΩçÁΩÆ‰∏éËÆøÈóÆÊñπÂºè
       - [ÂàÜÊûêÊï∞ÊçÆÊòØÂê¶Â≠òÂú®‰∫éÂ∫îÁî®ËæπÁïå‰πãÂ§ñ]
       - [ÂàÜÊûêÂ∫îÁî®ÊòØÂê¶‰ªÖÂºïÁî®ÔºàËØªÂèñÔºâËØ•Êï∞ÊçÆÔºå‰∏çËøõË°åÁª¥Êä§]

3. ÂèçÂêëÈ™åËØÅ
   3.1 [ËÄÉËôëÁõ∏ÂèçÊÉÖÂÜµ]
   3.2 [È™åËØÅÊòØÂê¶ÊúâÈÅóÊºè]

4. ÊúÄÁªàÁªìËÆ∫
   [ÊòØ/Âê¶]"""

    got_prompt = """‰Ω†ÊòØ‰∏Ä‰∏™IFPUGÂäüËÉΩÁÇπÂàÜÊûê‰∏ìÂÆ∂„ÄÇËØ∑Âà§Êñ≠ÁªôÂÆöÁöÑÂäüËÉΩÁÇπÊòØÂê¶ÊûÑÊàêÂ§ñÈÉ®Êé•Âè£Êñá‰ª∂ÔºàEIFÔºâ„ÄÇ

[ÈúÄÊ±ÇÊñáÊ°£]
{requirement_text}

[ÂÄôÈÄâÂäüËÉΩÁÇπ]
ÂêçÁß∞Ôºö{candidate_name}

ËØ∑Êåâ‰ª•‰∏ãÊ≠•È™§ÂàÜÊûêÔºö

1. ÈúÄÊ±ÇÂàÜËß£
- ËØÜÂà´ÂÖ≥ÈîÆÊï∞ÊçÆÂÆû‰Ωì
- ÂàÜÊûêÊï∞ÊçÆÂÖ≥Á≥ª
- Ê†áÊ≥®Êï∞ÊçÆÊù•Ê∫êÂíåËÆøÈóÆÊñπÂºè

2. Â§öË∑ØÂæÑÈ™åËØÅ
Ë∑ØÂæÑ1Ôºö‰ªéÁî®Êà∑ËßÜËßí
- Êï∞ÊçÆÁªÑÊòØÂê¶Êª°Ë∂≥‰∏öÂä°ÈúÄÊ±Ç
- Áî®Êà∑ÊòØÂê¶ËÉΩËØÜÂà´Ê≠§Êï∞ÊçÆÁªÑ

Ë∑ØÂæÑ2Ôºö‰ªéÁ≥ªÁªüËßÜËßí
- Êï∞ÊçÆÊòØÂê¶Â≠òÂú®‰∫éÂ∫îÁî®ËæπÁïå‰πãÂ§ñ
- Â∫îÁî®ÊòØÂê¶‰ªÖÂºïÁî®ÔºàËØªÂèñÔºâËØ•Êï∞ÊçÆÔºå‰∏çËøõË°åÂ¢ûÂà†ÊîπÊìç‰Ωú

Ë∑ØÂæÑ3Ôºö‰ªéIFPUGËßÑÂàôËßÜËßí
- Ê£ÄÊü•ÊòØÂê¶Á¨¶ÂêàEIFÂÆö‰πâ
- È™åËØÅÊòØÂê¶Êª°Ë∂≥ÊâÄÊúâÊù°‰ª∂

3. ÁªìÊûúÂêàÂπ∂
- ÁªºÂêàÂêÑË∑ØÂæÑÁªìÊûú
- Â§ÑÁêÜÂèØËÉΩÁöÑÂÜ≤Á™Å
- ÂæóÂá∫ÊúÄÁªàÂà§Êñ≠

ÊúÄÁªàÁ≠îÊ°àÔºö[ÊòØ/Âê¶]"""

    tot_improve_prompt = """‰Ω†ÊòØ‰∏Ä‰∏™IFPUGÂäüËÉΩÁÇπÂàÜÊûê‰∏ìÂÆ∂„ÄÇÂü∫‰∫é‰πãÂâçÁöÑÂàÜÊûêÁªìÊûúËøõË°åÊîπËøõÔºö

‰πãÂâçÁöÑÂà§Êñ≠Ôºö{current}

[ÈúÄÊ±ÇÊñáÊ°£]
{requirement_text}

[ÂÄôÈÄâÂäüËÉΩÁÇπ]
ÂêçÁß∞Ôºö{candidate_name}

ËØ∑Âü∫‰∫é‰πãÂâçÁöÑÂà§Êñ≠ËøõË°åÊîπËøõÔºö
1. ÂàÜÊûê‰πãÂâçÂà§Êñ≠ÁöÑ‰ºòÁÇπ
2. ÊâæÂá∫ÂèØËÉΩÁöÑÈóÆÈ¢òÊàñÈÅóÊºè
3. ÈáçÊñ∞Ê£ÄÊü•EIFÁöÑ‰∏â‰∏™ÂÖ≥ÈîÆÊù°‰ª∂
4. ÁªôÂá∫ÊîπËøõÂêéÁöÑÂà§Êñ≠

ÊúÄÁªàÁ≠îÊ°àÔºö[ÊòØ/Âê¶]"""

    perspective_prompt = """‰Ω†ÊòØ‰∏Ä‰∏™IFPUGÂäüËÉΩÁÇπÂàÜÊûê‰∏ìÂÆ∂„ÄÇËØ∑‰ªé{perspective}ÂàÜÊûêÊ≠§ÂäüËÉΩÁÇπÊòØÂê¶ÊûÑÊàêEIF„ÄÇ

[ÈúÄÊ±ÇÊñáÊ°£]
{requirement_text}

[ÂÄôÈÄâÂäüËÉΩÁÇπ]
ÂêçÁß∞Ôºö{candidate_name}

[ÂàÜÊûêËßÜËßíËØ¥Êòé]
Áî®Êà∑ËßÜËßí - ÂÖ≥Ê≥®Ôºö
- Êï∞ÊçÆÁªÑÊòØÂê¶ÈÄªËæë‰∏äÁã¨Á´ã‰∏îÁî®Êà∑ÂèØËØÜÂà´
- Êï∞ÊçÆÁªÑÊòØÂê¶ËÉΩÊª°Ë∂≥ÁâπÂÆöÁöÑ‰∏öÂä°ÈúÄÊ±Ç
- Êï∞ÊçÆÁªÑÂØπÁî®Êà∑ÊòØÂê¶ÊúâÂÆûÈôÖ‰∏öÂä°‰ª∑ÂÄº

Á≥ªÁªüËßÜËßí - ÂÖ≥Ê≥®Ôºö
- Êï∞ÊçÆÊòØÂê¶Áâ©ÁêÜ/ÈÄªËæë‰∏äÂ≠òÂú®‰∫éÂΩìÂâçÂ∫îÁî®‰πãÂ§ñ
- Â∫îÁî®ÊòØÂê¶‰ªÖÂºïÁî®ÔºàËØªÂèñÔºâËØ•Êï∞ÊçÆÔºå‰∏çËøõË°åÂ¢ûÂà†ÊîπÊìç‰Ωú
- Êï∞ÊçÆÊòØÂê¶Áî±ÂÖ∂‰ªñÂ∫îÁî®ÊàñÁ≥ªÁªüÁª¥Êä§

IFPUGËßÑÂàôËßÜËßí - ÂÖ≥Ê≥®Ôºö
- ÊòØÂê¶Êª°Ë∂≥EIFÁöÑÊâÄÊúâÂøÖË¶ÅÊù°‰ª∂ÔºàÈÄªËæëÁã¨Á´ã„ÄÅÂ§ñÈÉ®Â≠òÂÇ®„ÄÅÂè™ËØªÂºïÁî®Ôºâ
- ÊòØÂê¶Â≠òÂú®Âèç‰æãÊàñ‰æãÂ§ñÊÉÖÂÜµ
- ÊòØÂê¶Á¨¶ÂêàIFPUGÁöÑÊúÄ‰Ω≥ÂÆûË∑µ

[ÂàÜÊûêÊ≠•È™§]
1. ‰ªîÁªÜÂÆ°ËßÜÂÄôÈÄâÂäüËÉΩÁÇπÁöÑÊûÑÊàêË¶ÅÁ¥†
2. Âàó‰∏æÊîØÊåÅÂíåÂèçÂØπÁöÑÂÖ∑‰ΩìËØÅÊçÆ
3. ËÄÉËôëÊòØÂê¶Â≠òÂú®Âèç‰æãÊàñÁâπÊÆäÊÉÖÂÜµ
4. ÁªôÂá∫ËØ•ËßÜËßí‰∏ãÁöÑÊúÄÁªàÂà§Êñ≠

[ËæìÂá∫Ê†ºÂºè]
ÂàÜÊûêËøáÁ®ãÔºö
1. ÊûÑÊàêË¶ÅÁ¥†ÂàÜÊûêÔºö
   [ËØ¶ÁªÜÂàÜÊûêÂÜÖÂÆπ]

2. ÊîØÊåÅËØÅÊçÆÔºö
   - [ÂàóÂá∫ÂÖ∑‰ΩìÊîØÊåÅËØÅÊçÆ]

3. ÂèçÂØπËØÅÊçÆÔºö
   - [ÂàóÂá∫ÂÖ∑‰ΩìÂèçÂØπËØÅÊçÆ]

4. ÁâπÊÆäÊÉÖÂÜµËÄÉËôëÔºö
   [ÂàÜÊûêÊòØÂê¶Â≠òÂú®Âèç‰æãÊàñÁâπÊÆäÊÉÖÂÜµ]

5. ÁªìËÆ∫Ôºö
   [ÊÄªÁªìÊÄßÂàÜÊûê]

ËØ•ËßÜËßíÁöÑÂà§Êñ≠Ôºö[ÊòØ/Âê¶]"""

    merge_prompt = """‰Ω†ÊòØ‰∏Ä‰∏™IFPUGÂäüËÉΩÁÇπÂàÜÊûê‰∏ìÂÆ∂„ÄÇËØ∑ÁªºÂêà‰ª•‰∏ã‰∏â‰∏™ËßÜËßíÁöÑÂàÜÊûêÁªìÊûúÔºåÂà§Êñ≠Ê≠§ÂäüËÉΩÁÇπÊòØÂê¶ÊûÑÊàêEIF„ÄÇ

[ÈúÄÊ±ÇÊñáÊ°£]
{requirement_text}

[ÂÄôÈÄâÂäüËÉΩÁÇπ]
ÂêçÁß∞Ôºö{candidate_name}

[ÂêÑËßÜËßíÂàÜÊûêÁªìÊûú]
Áî®Êà∑ËßÜËßíÂàÜÊûêÔºö
{user_perspective}

Á≥ªÁªüËßÜËßíÂàÜÊûêÔºö
{system_perspective}

IFPUGËßÑÂàôËßÜËßíÂàÜÊûêÔºö
{ifpug_perspective}

[ÂàÜÊûêË¶ÅÊ±Ç]
1. ÂøÖÈ°ªÂêåÊó∂Êª°Ë∂≥‰ª•‰∏ãÊâÄÊúâÊù°‰ª∂ÊâçËÉΩÂà§ÂÆö‰∏∫EIFÔºö
   - ÈÄªËæë‰∏äÁã¨Á´ã‰∏îÁî®Êà∑ÂèØËØÜÂà´ÁöÑÊï∞ÊçÆÁªÑ
   - Ë¢´ÂΩìÂâçÂ∫îÁî®ÂºïÁî®Ôºå‰ΩÜÁâ©ÁêÜ/ÈÄªËæë‰∏äÂ≠òÂú®‰∫éÂΩìÂâçÂ∫îÁî®‰πãÂ§ñ
   - ‰∏çÁî±ÂΩìÂâçÂ∫îÁî®ËøõË°åÁª¥Êä§ÔºàÂç≥Âè™ËØªÔºå‰∏çÂ¢ûÂà†ÊîπÔºâ

2. Â≠òÂú®‰ª•‰∏ã‰ªª‰∏ÄÊÉÖÂÜµÂ∞±‰∏çËÉΩÂà§ÂÆö‰∏∫EIFÔºö
   - Êï∞ÊçÆÁî±ÂΩìÂâçÂ∫îÁî®Áª¥Êä§ÔºàÊúâÂ¢ûÂà†ÊîπÊìç‰ΩúÔºâ
   - Êï∞ÊçÆÂ≠òÂÇ®Âú®Â∫îÁî®ËæπÁïåÂÜÖ
   - Êï∞ÊçÆ‰∏çÊòØÈÄªËæëÁã¨Á´ãÁöÑ
   - ‰∏çÁ¨¶ÂêàIFPUGËßÑÂàôÁöÑË¶ÅÊ±Ç

ËØ∑Êåâ‰ª•‰∏ãÊ≠•È™§ÁªºÂêàÂàÜÊûêÔºö
1. ÂàÜÂà´ÊÄªÁªìÂêÑËßÜËßíÁöÑÂÖ≥ÈîÆÂèëÁé∞
2. Ê£ÄÊü•ÊòØÂê¶Êª°Ë∂≥ÊâÄÊúâÂøÖË¶ÅÊù°‰ª∂
3. Ê£ÄÊü•ÊòØÂê¶Â≠òÂú®‰ªª‰ΩïÊéíÈô§Êù°‰ª∂
4. ÊùÉË°°‰∏çÂêåËßÜËßíÁöÑËßÇÁÇπ
5. ÂæóÂá∫ÊúÄÁªàÂà§Êñ≠

ËØ∑ËæìÂá∫Ôºö
1. ÂêÑËßÜËßíÂÖ≥ÈîÆÂèëÁé∞Ôºö
   Áî®Êà∑ËßÜËßíÔºö[ÂÖ≥ÈîÆÂèëÁé∞]
   Á≥ªÁªüËßÜËßíÔºö[ÂÖ≥ÈîÆÂèëÁé∞]
   IFPUGËßÑÂàôËßÜËßíÔºö[ÂÖ≥ÈîÆÂèëÁé∞]

2. ÂøÖË¶ÅÊù°‰ª∂Ê£ÄÊü•Ôºö
   - ÈÄªËæë‰∏äÁã¨Á´ã‰∏îÁî®Êà∑ÂèØËØÜÂà´Ôºö[ÊòØ/Âê¶] - [ÁêÜÁî±]
   - Â≠òÂú®‰∫éÂ∫îÁî®ËæπÁïå‰πãÂ§ñÔºö[ÊòØ/Âê¶] - [ÁêÜÁî±]
   - ‰ªÖÂºïÁî®‰∏çÁª¥Êä§ÔºàÂè™ËØªÔºâÔºö[ÊòØ/Âê¶] - [ÁêÜÁî±]

3. ÊéíÈô§Êù°‰ª∂Ê£ÄÊü•Ôºö
   [ÂàóÂá∫ÂèëÁé∞ÁöÑ‰ªª‰ΩïÊéíÈô§Êù°‰ª∂]

4. ÁªºÂêàÂàÜÊûêÔºö
   [ËØ¶ÁªÜÁöÑÊùÉË°°ÂàÜÊûê]

ÊúÄÁªàÂà§Êñ≠Ôºö[ÊòØ/Âê¶]"""

    def generate_prompt(self, num_branches: int, current: str, method: str, **kwargs) -> str:
        """
        Generate a generate prompt for the language model.

        :param num_branches: The number of responses the prompt should ask the LM to generate.
        :type num_branches: int
        :param current: Intermediate solution.
        :type current: str
        :param method: Method for which the generate prompt is generated.
        :type method: str
        :param kwargs: Additional keyword arguments.
        :return: The generate prompt.
        :rtype: str
        :raise AssertionError: If the requested number of branches is not one.
        """
        assert num_branches == 1, "Branching should be done via multiple requests."
        
        # Ê∑ªÂä†Ë∞ÉËØïÊó•Âøó
        logging.debug(f"Method: {method}")
        logging.debug(f"Current state: {kwargs}")
        
        if method.startswith("io"):
            return self.io_prompt.format(
                requirement_text=kwargs["requirement_text"],
                candidate_name=kwargs["candidate_name"]
            )
        elif method.startswith("cot"):
            return self.cot_prompt.format(
                requirement_text=kwargs["requirement_text"],
                candidate_name=kwargs["candidate_name"]
            )
        elif method.startswith("tot"):
            if current is None or current == "":
                return self.tot_prompt.format(
                    requirement_text=kwargs["requirement_text"],
                    candidate_name=kwargs["candidate_name"]
                )
            return self.tot_improve_prompt.format(
                current=current,
                requirement_text=kwargs["requirement_text"],
                candidate_name=kwargs["candidate_name"]
            )
        elif method.startswith("got"):
            # Ê£ÄÊü•Áä∂ÊÄÅ‰∏≠ÁöÑphaseÂíåperspective
            if "phase" in kwargs and kwargs["phase"] == "analysis" and "perspective" in kwargs:
                logging.debug(f"Using perspective prompt for {kwargs['perspective']}")
                return self.perspective_prompt.format(
                    perspective=kwargs["perspective"],
                    requirement_text=kwargs["requirement_text"],
                    candidate_name=kwargs["candidate_name"]
                )
            elif "phase" in kwargs and kwargs["phase"] == "merge" and kwargs.get("merge_perspectives"):
                logging.debug("Using merge prompt")
                return self.merge_prompt.format(
                    requirement_text=kwargs["requirement_text"],
                    candidate_name=kwargs["candidate_name"],
                    user_perspective=kwargs.get("user_perspective", ""),
                    system_perspective=kwargs.get("system_perspective", ""),
                    ifpug_perspective=kwargs.get("ifpug_perspective", "")
                )
            else:
                logging.debug("Using default got prompt")
                return self.got_prompt.format(
                    requirement_text=kwargs["requirement_text"],
                    candidate_name=kwargs["candidate_name"]
                )

    def aggregation_prompt(self, state_dicts: List[Dict], **kwargs) -> str:
        """
        Generate an aggregation prompt for the language model.

        :param state_dicts: The thought states that should be aggregated.
        :type state_dicts: List[Dict]
        :param kwargs: Additional keyword arguments.
        :return: The aggregation prompt.
        :rtype: str
        """
        pass

    def improve_prompt(self, current: str, aggr1: str, aggr2: str, **kwargs) -> str:
        """
        Generate an improve prompt for the language model.

        :param current: Intermediate solution.
        :type current: str
        :param aggr1: Partially solution 1 before aggregation.
        :type aggr1: str
        :param aggr2: Partially solution 2 before aggregation.
        :type aggr2: str
        :param kwargs: Additional keyword arguments.
        :return: The improve prompt.
        :rtype: str
        """
        pass

    def validation_prompt(self, **kwargs) -> str:
        """
        Generate a validation prompt for the language model.

        :param kwargs: Additional keyword arguments.
        :return: The validation prompt.
        :rtype: str
        """
        pass

    def score_prompt(self, state_dicts: List[Dict], **kwargs) -> str:
        """
        Generate a score prompt for the language model.

        :param state_dicts: The thought states that should be scored,
                            if more than one, they should be scored together.
        :type state_dicts: List[Dict]
        :param kwargs: Additional keyword arguments.
        :return: The score prompt.
        :rtype: str
        """
        pass

class FunctionPointParser(parser.Parser):
    """
    FunctionPointParser provides the parsing of language model responses specific to the
    function point assessment example.

    Inherits from the Parser class and implements its abstract methods.
    """

    def extract_answer(self, text: str) -> str:
        """
        ‰ªéÊñáÊú¨‰∏≠ÊèêÂèñÁ≠îÊ°àÔºàÊòØ/Âê¶Ôºâ„ÄÇ

        :param text: ÂåÖÂê´Á≠îÊ°àÁöÑÊñáÊú¨
        :type text: str
        :return: ÊèêÂèñÂá∫ÁöÑÁ≠îÊ°àÔºàÊòØ/Âê¶Ôºâ
        :rtype: str
        """
        # Â∞ùËØï‰∏çÂêåÁöÑÁ≠îÊ°àÊ†ºÂºè
        patterns = [
            r'ÊúÄÁªàÂà§Êñ≠Ôºö\[ÊòØ/Âê¶\]',  # Ê†áÂáÜÊ†ºÂºè
            r'ÊúÄÁªàÂà§Êñ≠Ôºö.*?(?:ÊòØ|Âê¶)',  # Â∏¶‰ªªÊÑèÂ≠óÁ¨¶ÁöÑÊ†ºÂºè
            r'ËØ•ËßÜËßíÁöÑÂà§Êñ≠Ôºö\[ÊòØ/Âê¶\]',  # ËßÜËßíÂàÜÊûêÊ†ºÂºè
            r'ËØ•ËßÜËßíÁöÑÂà§Êñ≠Ôºö.*?(?:ÊòØ|Âê¶)',  # Â∏¶‰ªªÊÑèÂ≠óÁ¨¶ÁöÑËßÜËßíÂàÜÊûêÊ†ºÂºè
            r'ÊúÄÁªàÁ≠îÊ°àÔºö\[ÊòØ/Âê¶\]',  # Âè¶‰∏ÄÁßçÊ†áÂáÜÊ†ºÂºè
            r'ÊúÄÁªàÁ≠îÊ°àÔºö.*?(?:ÊòØ|Âê¶)',  # Â∏¶‰ªªÊÑèÂ≠óÁ¨¶ÁöÑÂè¶‰∏ÄÁßçÊ†ºÂºè
            r'Âà§Êñ≠Ôºö.*?(?:ÊòØ|Âê¶)',  # ÁÆÄÂçïÊ†ºÂºè
            r'ÁªìËÆ∫Ôºö.*?(?:ÊòØ|Âê¶)',  # ÁªìËÆ∫Ê†ºÂºè
            r'\*\*(?:ÊòØ|Âê¶)\*\*',  # MarkdownÂä†Á≤óÊ†ºÂºè
            r'ÊúÄÁªàÂà§Êñ≠Ôºö\*\*(?:ÊòØ|Âê¶)\*\*',  # Â∏¶Âä†Á≤óÁöÑÊúÄÁªàÂà§Êñ≠Ê†ºÂºè
            r'(?:ÊòØ|Âê¶)'  # ÊúÄÁÆÄÂçïÁöÑÊ†ºÂºèÔºàÊúÄÂêéÂ∞ùËØïÔºâ
        ]
        
        # ÂéªÈô§ÊâÄÊúâÊç¢Ë°åÁ¨¶Ôºå‰æø‰∫éÂåπÈÖç
        text = text.replace('\n', ' ')
        
        for pattern in patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                # ÊèêÂèñÂåπÈÖçÊñáÊú¨‰∏≠ÁöÑ"ÊòØ"Êàñ"Âê¶"
                answer = re.search(r'(?:ÊòØ|Âê¶)', match.group())
                if answer:
                    return answer.group()
        
        # Â¶ÇÊûúÊ≤°ÊúâÊâæÂà∞‰ªª‰ΩïÂåπÈÖçÔºåËøîÂõûÈªòËÆ§ÂÄº
        logging.warning(f"No answer found in text: {text}")
        return "Âê¶"  # ÈªòËÆ§ËøîÂõûÂê¶

    def parse_generate_answer(self, state: Dict, texts: List[str]) -> List[Dict]:
        """
        Parse the response from the language model for a generate prompt.

        :param state: The thought state used to generate the prompt.
        :type state: Dict
        :param texts: The responses to the prompt from the language model.
        :type texts: List[str]
        :return: The new thought states after parsing the responses from the language model.
        :rtype: List[Dict]
        """
        new_states = []
        for text in texts:
            try:
                new_state = state.copy()
                
                # ‰øùÂ≠òÂéüÂßãÂõûÁ≠î
                new_state["current"] = text
                
                # ÊèêÂèñÁ≠îÊ°àÂπ∂ËΩ¨Êç¢‰∏∫Â∏ÉÂ∞îÁ±ªÂûã
                answer = self.extract_answer(text)
                new_state["final_answer"] = (answer == "ÊòØ")
                
                # Ê†πÊçÆ‰∏çÂêåÈò∂ÊÆµÂ≠òÂÇ®ÂàÜÊûêÁªìÊûú
                if "perspective" in state:
                    # ËßÜËßíÂàÜÊûêÈò∂ÊÆµ
                    perspective = state["perspective"]
                    new_state[f"{perspective}_analysis"] = text
                    # Â∞ÜÂàÜÊûêÁªìÊûú‰πüÂ≠òÂÇ®Âà∞ÂêàÂπ∂Èò∂ÊÆµ‰ºöÁî®Âà∞ÁöÑÈîÆ‰∏≠
                    if perspective == "Áî®Êà∑ËßÜËßí":
                        new_state["user_perspective"] = text
                    elif perspective == "Á≥ªÁªüËßÜËßí":
                        new_state["system_perspective"] = text
                    elif perspective == "IFPUGËßÑÂàôËßÜËßí":
                        new_state["ifpug_perspective"] = text
                elif "merge_perspectives" in state:
                    # ÂêàÂπ∂Èò∂ÊÆµ
                    new_state["merged_analysis"] = text
                
                new_states.append(new_state)
            except Exception as e:
                logging.error(f"Could not parse answer: {text}. Error: {e}")
                # ÂèëÁîüÈîôËØØÊó∂Ê∑ªÂä†‰∏Ä‰∏™ÈªòËÆ§Áä∂ÊÄÅ
                default_state = state.copy()
                default_state["current"] = text
                default_state["final_answer"] = False  # ÈªòËÆ§‰∏∫Âê¶
                default_state["parse_error"] = str(e)
                new_states.append(default_state)
        return new_states

    def parse_aggregation_answer(self, states: List[Dict], texts: List[str]) -> Union[Dict, List[Dict]]:
        """
        Parse the response from the language model for an aggregation prompt.

        :param states: The thought states used to generate the prompt.
        :type states: List[Dict]
        :param texts: The responses to the prompt from the language model.
        :type texts: List[str]
        :return: The new thought states after parsing the respones from the language model.
        :rtype: Union[Dict, List[Dict]]
        """
        pass

    def parse_improve_answer(self, state: Dict, texts: List[str]) -> Dict:
        """
        Parse the response from the language model for an improve prompt.

        :param state: The thought state used to generate the prompt.
        :type state: Dict
        :param texts: The responses to the prompt from the language model.
        :type texts: List[str]
        :return: The new thought state after parsing the responses from the language model.
        :rtype: Dict
        """
        pass

    def parse_validation_answer(self, state: Dict, texts: List[str]) -> bool:
        """
        Parse the response from the language model for a validation prompt.

        :param state: The thought state used to generate the prompt.
        :type state: Dict
        :param texts: The responses to the prompt from the language model.
        :type texts: List[str]
        :return: Whether the thought state is valid or not.
        :rtype: bool
        """
        pass

    def parse_score_answer(self, states: List[Dict], texts: List[str]) -> List[float]:
        """
        Parse the response from the language model for a score prompt.

        :param states: The thought states used to generate the prompt.
        :type states: List[Dict]
        :param texts: The responses to the prompt from the language model.
        :type texts: List[str]
        :return: The scores for the thought states.
        :rtype: List[float]
        """
        pass

def io() -> operations.GraphOfOperations:
    """
    Generates the Graph of Operations for the IO method.

    :return: Graph of Operations
    :rtype: GraphOfOperations
    """
    operations_graph = operations.GraphOfOperations()

    operations_graph.append_operation(operations.Generate(1, 1))
    operations_graph.append_operation(operations.Score(1, False, score_assessment))
    operations_graph.append_operation(operations.GroundTruth(test_eif_assessment))

    return operations_graph

def cot() -> operations.GraphOfOperations:
    """
    Generates the Graph of Operations for the CoT method.

    :return: Graph of Operations
    :rtype: GraphOfOperations
    """
    operations_graph = operations.GraphOfOperations()

    operations_graph.append_operation(operations.Generate(1, 1))
    operations_graph.append_operation(operations.Score(1, False, score_assessment))
    operations_graph.append_operation(operations.GroundTruth(test_eif_assessment))

    return operations_graph

def tot() -> operations.GraphOfOperations:
    """
    Generates the Graph of Operations for the ToT method.

    :return: Graph of Operations
    :rtype: GraphOfOperations
    """
    operations_graph = operations.GraphOfOperations()

    # ÁõÆÂâçÊúâÈóÆÈ¢òÔºådeepseek‰∏çÊîØÊåÅËØïÁî®ÂèÇÊï∞nÁîüÊàêÂ§ö‰∏™ÂõûÂ§çchoices
    operations_graph.append_operation(operations.Generate(1, 1))
    operations_graph.append_operation(operations.Score(1, False, score_assessment))
    keep_best_1 = operations.KeepBestN(1, True)  # True: ÈÄâÊã©ÊúÄÈ´òÂàÜÊï∞
    operations_graph.append_operation(keep_best_1)

    for _ in range(3):
        operations_graph.append_operation(operations.Generate(1, 1))
        operations_graph.append_operation(operations.Score(1, False, score_assessment))
        keep_best_2 = operations.KeepBestN(1, True)  # True: ÈÄâÊã©ÊúÄÈ´òÂàÜÊï∞
        keep_best_2.add_predecessor(keep_best_1)
        operations_graph.append_operation(keep_best_2)
        keep_best_1 = keep_best_2

    operations_graph.append_operation(operations.KeepBestN(1, True))  # True: ÈÄâÊã©ÊúÄÈ´òÂàÜÊï∞
    operations_graph.append_operation(operations.GroundTruth(test_eif_assessment))

    return operations_graph

def got() -> operations.GraphOfOperations:
    """
    Generates the Graph of Operations for the GoT method.
    ‰ΩøÁî®ÂõæÁªìÊûÑÊù•ÂàÜÊûêEIFÂà§Êñ≠ÈóÆÈ¢òÔºö
    1. ‰ªé‰∏â‰∏™‰∏çÂêåËßÜËßíÂàÜÊûêÔºàÁî®Êà∑ËßÜËßí„ÄÅÁ≥ªÁªüËßÜËßí„ÄÅIFPUGËßÑÂàôËßÜËßíÔºâ
    2. ÊØè‰∏™ËßÜËßíÁîüÊàêÂ§ö‰∏™ÊÄùË∑ØÂπ∂ÈÄâÊã©ÊúÄ‰Ω≥
    3. ÂêàÂπ∂ÂíåÈ™åËØÅÁªìÊûú
    """
    operations_graph = operations.GraphOfOperations()

    # 1. ‰ªé‰∏â‰∏™‰∏çÂêåËßÜËßíËøõË°åÂàÜÊûê
    perspectives = ["Áî®Êà∑ËßÜËßí", "Á≥ªÁªüËßÜËßí", "IFPUGËßÑÂàôËßÜËßí"]
    perspective_results = []
    
    for perspective in perspectives:
        # 1.1 ÁîüÊàêËØ•ËßÜËßíÁöÑÂàÜÊûê
        generate = operations.Generate(1, 1)
        # Â∞ÜËßÜËßí‰ø°ÊÅØÊ∑ªÂä†Âà∞ÂàùÂßãÁä∂ÊÄÅ‰∏≠
        generate.initial_state = {
            "perspective": perspective,
            "phase": "analysis"
        }
        operations_graph.add_operation(generate)

        # 1.2 ËØÑÂàÜ
        score = operations.Score(1, False, score_assessment)
        score.add_predecessor(generate)
        operations_graph.add_operation(score)
        
        # 1.3 ‰øùÁïôÊúÄ‰Ω≥ÁªìÊûú
        keep_best = operations.KeepBestN(1, True)
        keep_best.add_predecessor(score)
        operations_graph.add_operation(keep_best)
        
        perspective_results.append(keep_best)

    # 2. ÂêàÂπ∂‰∏â‰∏™ËßÜËßíÁöÑÁªìÊûú
    merge = operations.Generate(1, 1)
    # ËÆæÁΩÆÂêàÂπ∂Èò∂ÊÆµÁöÑÁä∂ÊÄÅ
    merge.initial_state = {
        "phase": "merge",
        "merge_perspectives": True
    }
    for result in perspective_results:
        merge.add_predecessor(result)
    operations_graph.add_operation(merge)

    # 3. ËØÑÂàÜÂíåÈÄâÊã©ÊúÄÁªàÁªìÊûú
    final_score = operations.Score(1, False, score_assessment)
    final_score.add_predecessor(merge)
    operations_graph.add_operation(final_score)

    final_keep = operations.KeepBestN(1, True)
    final_keep.add_predecessor(final_score)
    operations_graph.add_operation(final_keep)

    # 4. È™åËØÅ
    operations_graph.append_operation(operations.GroundTruth(test_eif_assessment))

    return operations_graph

def run(data_ids: List[int], methods: List[Callable[[], operations.GraphOfOperations]], budget: float, lm_name: str) -> float:
    """
    Controller function that executes each specified method for each specified
    sample while the budget is not exhausted.

    :param data_ids: Indices of the sample to be run.
    :type data_ids: List[int]
    :param methods: List of functions to generate Graphs of Operations.
    :type methods: Each function generates a Graph of Operation.
    :param budget: Language model budget for the execution in dollars.
    :type budget: float
    :param lm_name: Name of the language model to be used.
    :type lm_name: str
    :return: Spent budget in dollars.
    :rtype: float
    """
    orig_budget = budget
    data_path = os.path.join(os.path.dirname(__file__), "eif_samples.csv")
    data = []
    with open(data_path, "r", encoding="gbk") as f:  # ‰ΩøÁî® GBK ÁºñÁ†Å
        reader = csv.reader(f)
        next(reader)  # Skip header
        for row in reader:
            data.append([int(row[0]), row[1], row[2], row[3] == "TRUE"])

    if data_ids is None or len(data_ids) == 0:
        data_ids = list(range(len(data)))
    selected_data = [data[i] for i in data_ids]

    results_dir = os.path.join(os.path.dirname(__file__), "results")

    if not os.path.exists(results_dir):
        os.makedirs(results_dir)
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    extra_info = f"{lm_name}_{'-'.join([method.__name__ for method in methods])}"
    folder_name = f"{extra_info}_{timestamp}"
    results_folder = os.path.join(results_dir, folder_name)
    os.makedirs(results_folder)

    config = {
        "data": selected_data,
        "methods": [method.__name__ for method in methods],
        "lm": lm_name,
        "budget": budget,
    }
    with open(os.path.join(results_folder, "config.json"), "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)

    logging.basicConfig(
        filename=os.path.join(results_folder, "log.log"),
        filemode="w",
        format="%(name)s - %(levelname)s - %(message)s",
        level=logging.DEBUG,
        encoding="utf-8"
    )

    for method in methods:
        # create a results directory for the method
        os.makedirs(os.path.join(results_folder, method.__name__))

    for data in selected_data:
        logging.info(f"Running data {data[0]}: {data[1]}")
        if budget <= 0.0:
            logging.error(f"Budget has been depleted, stopping. Data {data[0]} has not been run.")
            break
        for method in methods:
            logging.info(f"Running method {method.__name__}")
            logging.info(f"Budget left: {budget}")
            if budget <= 0.0:
                logging.error(f"Budget has been depleted, stopping. Method {method.__name__} has not been run.")
                break

            lm = language_models.ChatGPT(
                os.path.join(
                    os.path.dirname(__file__),
                    "../../graph_of_thoughts/language_models/config.json",
                ),
                model_name=lm_name,
                cache=True,
            )
            operations_graph = method()
            executor = controller.Controller(
                lm,
                operations_graph,
                FunctionPointPrompter(),
                FunctionPointParser(),
                {
                    "requirement_text": data[2],
                    "candidate_name": data[1],
                    "ground_truth": data[3],
                    "current": "",
                    "method": method.__name__,
                },
            )
            try:
                executor.run()
            except Exception as e:
                logging.error(f"Exception: {e}")
            path = os.path.join(
                results_folder,
                method.__name__,
                f"{data[0]}.json",
            )
            executor.output_graph(path)
            budget -= lm.cost

    return orig_budget - budget

if __name__ == "__main__":
    """
    Input (x)   : ÈúÄÊ±ÇÊñáÊ°£ÂíåÂÄôÈÄâÂäüËÉΩÁÇπÂêçÁß∞
    Output (y)  : Âà§Êñ≠ÁªìÊûúÔºàÊòØ/Âê¶Ôºâ
    Correct     : y == Ê†áÂáÜÁ≠îÊ°à
    Input Example:
        ÈúÄÊ±ÇÊñáÊ°£Ôºö‰∫∫ÂäõËµÑÊ∫êÁÆ°ÁêÜÁ≥ªÁªü - ËÅå‰Ωç‰ø°ÊÅØÁÆ°ÁêÜÊ®°Âùó...
        ÂÄôÈÄâÂäüËÉΩÁÇπÔºöJob information
    Output Example:
        ÊòØ
    """
    # ËÆæÁΩÆÊéßÂà∂Âè∞Êó•ÂøóËæìÂá∫
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),  # ÊéßÂà∂Âè∞ËæìÂá∫
            logging.FileHandler('experiment.log', encoding='utf-8')  # Êñá‰ª∂ËæìÂá∫
        ]
    )
    
    print("üöÄ ÂºÄÂßãËøêË°åÂäüËÉΩÁÇπËØÑ‰º∞ÂÆûÈ™å...")
    print("=" * 50)
    
    budget = 5
    samples = [0,1,2,3,4,5,6,7,8,9]  # Âè™‰ΩøÁî®Á¨¨‰∏Ä‰∏™Ê†∑Êú¨ËøõË°åÊµãËØï
    approaches = [tot]  # ‰ΩøÁî®ÊâÄÊúâÊñπÊ≥ïËøõË°åÊµãËØï

    print(f"üìä ÂÆûÈ™åÈÖçÁΩÆ:")
    print(f"   - È¢ÑÁÆó: ${budget}")
    print(f"   - Ê†∑Êú¨Êï∞Èáè: {len(samples)}")
    print(f"   - ÊñπÊ≥ï: {[method.__name__ for method in approaches]}")
    print(f"   - Ê®°Âûã: qwen3-235b")
    print("=" * 50)


    spent = run(samples, approaches, budget, "qwen3-235b")

    print("=" * 50)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         
    print(f"‚úÖ ÂÆûÈ™åÂÆåÊàêÔºÅ")
    print(f"üí∞ Ëä±Ë¥π: ${spent:.2f} / ${budget}")
    print(f"üìÅ ÁªìÊûú‰øùÂ≠òÂú®: results/ ÁõÆÂΩï")
    logging.info(f"Spent {spent} out of {budget} budget.") 