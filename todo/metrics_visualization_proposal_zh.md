
# 评估指标可视化改进提案 (Metrics Visualization & Interpretation)

**背景 (Context):**
在对实验结果（第 5.2 节）进行核验时，我们发现不同的推理范式（例如 CoT 与 IO）可能在**有效真阳性总数 ($M_{total}$)** 上完全一致，但在**宏平均精确率/召回率/F1 (Macro Precision/Recall/F1)** 上却存在显著差异。此外，我们注意到论文中第 4.2 节的定义（倾向于微平均）与第 5.2 节实际使用的计算方式（宏平均）存在不一致。

**核心洞察 (Insight):**

1.  **$M_{total}$ 的局限性**：
    *   $M_{total}$ 仅衡量了模型在整个数据集上提取出的“正确信息总量”。
    *   它无法反映模型是否产生了额外的噪音（误报），也无法体现模型在不同样本间表现的稳定性。

2.  **宏平均 (Macro Average) vs. 微平均 (Micro Average) 的区别**：
    *   **计算逻辑**：
        *   **微平均 (Micro)**：将所有样本的 TP, FP, FN 累加后计算一次 P/R。它关注的是“每一个被提取出的实体”的平均质量。
        *   **宏平均 (Macro)**：先独立计算每个测试样本的 P/R，然后求算数平均值。它赋予每个测试样本（无论其包含多少实体）同等的权重。
    *   **对“空样本”的处理差异**：
        *   在我们的测试集中，存在一部分**真值为空 (Ground Truth = [])** 的“空对空”样本。
        *   对于**表现完美（预测也为空）** 的模型：
            *   **微平均**：TP=0, FP=0, FN=0。该样本对分子分母均无贡献，相当于被忽略。
            *   **宏平均**：定义 Precision=1.0, Recall=1.0。该样本会显著拉高整体平均分，奖励了模型“正确保持沉默”的能力。
        *   对于**出现幻觉（预测出错误实体）** 的模型：
            *   **微平均**：FP增加，略微拉低整体 Precision。
            *   **宏平均**：该样本 Precision=0。这对整体平均分是**毁灭性的打击**（例如 0 和 1 的平均是 0.5，一下拉低很多）。
    *   **结论**：在我们的场景下，**宏平均 (Macro)** 更能体现模型的**鲁棒性 (Robustness)** 和**信噪比 (Signal-to-Noise Ratio)**，因为它严厉惩罚了在简单场景下的幻觉行为。

3.  **案例分析**：
    *   **DeepSeek CoT** ($M_{total}=1.80$) vs. **DeepSeek IO** ($M_{total}=1.80$)。
    *   两者抓取的有效信息量一样，但 CoT 的 F1 (63.3%) 远高于 IO (42.8%)。
    *   原因是 IO 范式在不需要提取的样本中产生了幻觉（Pred > Truth），导致这些样本的 Precision 为 0，拉低了宏平均；而 CoT 正确地什么都没做，拿到了 100% 的 Precision。

**改进计划 (Action Item):**
建议更新论文中的结果表格，在展示 $M_{total}$ 的基础上，增加 **预测总数 ($|Pred|$)** 和 **真值总数 ($|Truth|$)** 两列。

**建议表格结构 (Proposed Table Structure):**

| 模型 (Model) | $M_{total}$ | $|Pred|$ (预测数) | $|Truth|$ (真值数) | Precision (Macro) | Recall (Macro) | F1 Score (Macro) | Cost |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| DeepSeek CoT | 1.80 | 4 | 3 | 63.3% | 63.3% | 63.3% | $0.0123 |
| DeepSeek IO | 1.80 | 7 | 3 | 39.6% | 46.7% | 42.8% | $0.0065 |

**待办事项：修正第 4.2 节的指标定义 (Action Item: Fix Definition Discrepancy)**
我们需要修正 `mypaper/experiment/section_4.2_metrics.md` 中的公式定义，因为它们目前描述的是微平均，但我们的分析（以及第 5.2 节的表格）均基于宏平均。

*   **当前定义（隐含 Micro Average）**：
    *   $$ precision = \frac{M_{total}}{|Pred|} $$
    *   $$ recall = \frac{M_{total}}{|Truth|} $$
    *   这里的 $|Pred|$ 和 $|Truth|$ 指的是整个数据集的总数。

*   **需要修改为（Macro Average）**：
    *   应明确指出指标是“基于每个测试用例的得分的平均值”。
    *   $$ Precision_{macro} = \frac{1}{N} \sum_{i=1}^{N} Precision_i $$
    *   $$ Recall_{macro} = \frac{1}{N} \sum_{i=1}^{N} Recall_i $$
    *   其中 $N$ 是测试用例的数量。

**收益 (Benefits):**
*   **直观展示幻觉**：用户可以通过对比 $|Pred|$ 和 $|Truth|$ 一眼看出模型是否倾向于“胡言乱语”（如 IO 的 7 vs 3）。
*   **消除歧义**：修正第 4.2 节的定义可以确保方法论描述与实验结果的一致性。
*   **解释差异**：能够清晰地解释为什么两个 $M_{total}$ 相同的模型，其 F1 分数会有如此巨大的差异（强调了模型对噪音的控制能力）。
